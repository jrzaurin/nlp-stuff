{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I initially run 16 experiments: \n",
    "\n",
    "```bash\n",
    "python score.py --maxevals 50 --save\n",
    "python score.py --maxevals 50 --with_bigrams --save\n",
    "python score.py --packg spacy --maxevals 50 --save\n",
    "python score.py --packg spacy --maxevals 50 --with_bigrams --save\n",
    "\n",
    "python score.py --algo lda --n_topics 20 --with_cv --save\n",
    "python score.py --algo lda --n_topics 50 --with_cv --save\n",
    "python score.py --algo lda --n_topics 20 --with_bigrams --with_cv --save\n",
    "python score.py --algo lda --n_topics 50 --with_bigrams --with_cv --save\n",
    "python score.py --packg spacy --algo lda --n_topics 20 --with_cv --save\n",
    "python score.py --packg spacy --algo lda --n_topics 50 --with_cv --save\n",
    "python score.py --packg spacy --algo lda --n_topics 20 --with_bigrams --with_cv --save\n",
    "python score.py --packg spacy --algo lda --n_topics 50 --with_bigrams --with_cv --save\n",
    "\n",
    "python score.py --algo ensemb --n_topics 20 --with_cv --save\n",
    "python score.py --algo ensemb --n_topics 20 --with_bigrams --with_cv --save\n",
    "python score.py --packg spacy --algo ensemb --n_topics 20 --with_cv --save\n",
    "python score.py --packg spacy --algo ensemb --n_topics 20 --with_bigrams --with_cv --save\n",
    "```\n",
    "\n",
    "I have used `LDA`, `EnStop` (pLSA+`UMAP`) and `tfidf` with `nltk` and `spacy` tokenization (see `preprocessing.py`) with and without bigrams. \n",
    "\n",
    "All the code neccesary to run the experiments can be found in `score.py`. Is mostly this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import pdb\n",
    "\n",
    "from pathlib import Path\n",
    "# Note: I simply copied the `utils` dir into the notebooks dir so that I can run the next cell here\n",
    "from utils.lightgbm_optimizer import LGBOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "packg = 'nltk'           # nltk or spacy\n",
    "with_bigrams = False\n",
    "algo = 'lda'             # lda or ensemb\n",
    "n_topics = '20'          # 20 or 50 when lda, only 20 for ensemb\n",
    "with_cv = False          # hyperoptimize with cross validation\n",
    "is_unbalance = True      # set the lightgbm is_unbalance param to True/False\n",
    "with_focal_loss = False  # Use the Focal Loss (see here: https://github.com/jrzaurin/LightGBM-with-Focal-Loss)\n",
    "eval_with_metric = False # hyperoptimize using the F1 score or the CE Loss\n",
    "save = False             \n",
    "maxevals = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.61s/it, best loss: 0.9695638766845442]\n",
      "acc: 0.6034, f1 score: 0.5129, precision: 0.5108, recall: 0.6034\n",
      "confusion_matrix\n",
      "[[  483   110   355  1717]\n",
      " [  287   125   476  2154]\n",
      " [  287   113   572  4863]\n",
      " [  199    73   417 15633]]\n"
     ]
    }
   ],
   "source": [
    "FEAT_PATH = Path('../features')\n",
    "\n",
    "wbigram = 'bigram_' if with_bigrams else ''\n",
    "dataname = packg + '_tok_reviews_' + wbigram + algo\n",
    "if algo is not 'tfidf': dataname = dataname + '_' +  n_topics\n",
    "\n",
    "dtrain = pickle.load(open(FEAT_PATH/('train/'+ dataname+'_feat_tr.p'), 'rb'))\n",
    "dvalid = pickle.load(open(FEAT_PATH/('valid/'+ dataname+'_feat_val.p'), 'rb'))\n",
    "dtest  = pickle.load(open(FEAT_PATH/('test/' + dataname+'_feat_te.p'),  'rb'))\n",
    "\n",
    "opt = LGBOptimizer(\n",
    "    dataname,\n",
    "    dtrain,\n",
    "    dvalid,\n",
    "    dtest,\n",
    "    with_cv=with_cv,\n",
    "    is_unbalance=is_unbalance,\n",
    "    with_focal_loss=with_focal_loss,\n",
    "    eval_with_metric=eval_with_metric,\n",
    "    save=save)\n",
    "opt.optimize(maxevals=maxevals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first comment a bit on what happens within `LGBOptimizer` (all related code can be found in the `utils` module). There I use `hyperopt` and the following parameter space to optimize `LightGBM` hyper-parameters:\n",
    "\n",
    "```python\n",
    "space = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'num_boost_round': hp.quniform('num_boost_round', 50, 500, 20),\n",
    "    'num_leaves': hp.quniform('num_leaves', 31, 255, 4),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0.1, 10),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.01, 0.1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, 0.1),\n",
    "}\n",
    "```\n",
    "\n",
    " I think I could perhaps refine a bit more some of them, like using a slightly higher `learning_rate` (e.g 0.3) of a smaller `num_boost_round` (e.g. 10). I will leave that to future visits to this repo or to you, the reader ðŸ™‚. \n",
    "\n",
    "Within the class `LGBOptimizer`, one can run the optimization process with a number of options. For example, using cross validation and the F1 score. This will basically run the next piece of code:\n",
    "\n",
    "```python\n",
    "cv_result = lgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=params['num_boost_round'],\n",
    "    metrics='multi_logloss',\n",
    "    feval = lgb_f1 if self.eval_with_metric else None,\n",
    "    nfold=3,\n",
    "    stratified=True,\n",
    "    early_stopping_rounds=20)\n",
    "```\n",
    "\n",
    "where lgb_f1 is\n",
    "\n",
    "```python\n",
    "def lgb_f1_score(preds, lgbDataset, num_class):\n",
    "\t\"\"\"\n",
    "\tImplementation of the f1 score to be used as evaluation score for lightgbm\n",
    "\tParameters:\n",
    "\t-----------\n",
    "\tpreds: numpy.ndarray\n",
    "\t\tarray with the predictions\n",
    "\tlgbDataset: lightgbm.Dataset\n",
    "\t\"\"\"\n",
    "\tpreds = preds.reshape(-1, num_class, order='F')\n",
    "\tcat_preds = np.argmax(preds, axis=1)\n",
    "\ty_true = lgbDataset.get_label()\n",
    "\treturn 'f1', f1_score(y_true, cat_preds, average='weighted'), True\n",
    "\n",
    "```\n",
    "\n",
    "If you want to know more about custom losses and metrics for `LightGBM`, have a look to my repo [here](https://github.com/jrzaurin/LightGBM-with-Focal-Loss). Again, all the related code can be found in the `utils` module. \n",
    "\n",
    "So, if we choose to run 100 iterations, with cross validation and evaluate using the F1 score, for a set of features that is the results of using LDA with 20 topics, `LGBOptimizer` will take the train, validation and test datasets, merge the first two and run a 3 stratified-fold CV experiments on them. Once the best parameters have been found based on the resulting F1 score, it will then predict on the test set and compute the success metrics\n",
    "\n",
    "```python\n",
    "acc  = accuracy_score(self.lgtest.label, preds)\n",
    "f1   = f1_score(self.lgtest.label, preds, average='weighted')\n",
    "prec = precision_score(self.lgtest.label, preds, average='weighted')\n",
    "rec  = recall_score(self.lgtest.label, preds, average='weighted')\n",
    "cm   = confusion_matrix(self.lgtest.label, preds)\n",
    "```\n",
    "\n",
    "Have a look to the code in `utils` and the process will be pretty clear, I hope...and \"that's it\". Let's have a look to the results for the 16 experiments at the top of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../results/nltk_tok_reviews_bigram_tfidf_results_unb.p'),\n",
       " PosixPath('../results/nltk_tok_reviews_ensemb_20_results_unb.p'),\n",
       " PosixPath('../results/spacy_tok_reviews_ensemb_20_results_unb.p'),\n",
       " PosixPath('../results/spacy_tok_reviews_lda_50_results_unb.p'),\n",
       " PosixPath('../results/spacy_tok_reviews_bigram_lda_20_results_unb.p'),\n",
       " PosixPath('../results/spacy_tok_reviews_bigram_tfidf_results_unb.p'),\n",
       " PosixPath('../results/nltk_tok_reviews_bigram_lda_50_results_unb.p'),\n",
       " PosixPath('../results/spacy_tok_reviews_lda_20_results_unb.p'),\n",
       " PosixPath('../results/spacy_tok_reviews_bigram_ensemb_20_results_unb.p'),\n",
       " PosixPath('../results/nltk_tok_reviews_bigram_lda_20_results_unb.p'),\n",
       " PosixPath('../results/nltk_tok_reviews_tfidf_results_unb.p'),\n",
       " PosixPath('../results/nltk_tok_reviews_bigram_ensemb_20_results_unb.p'),\n",
       " PosixPath('../results/spacy_tok_reviews_bigram_lda_50_results_unb.p'),\n",
       " PosixPath('../results/nltk_tok_reviews_lda_20_results_unb.p'),\n",
       " PosixPath('../results/spacy_tok_reviews_tfidf_results_unb.p'),\n",
       " PosixPath('../results/nltk_tok_reviews_lda_50_results_unb.p')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULT_PATH = Path('../results/')\n",
    "results_fnames = list(RESULT_PATH.glob(\"*.p\"))\n",
    "results_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.681739879414298,\n",
       " 'f1': 0.6528736886562243,\n",
       " 'prec': 0.6454948246543382,\n",
       " 'rec': 0.681739879414298,\n",
       " 'cm': array([[ 1490,   430,   240,   505],\n",
       "        [  551,   781,   733,   977],\n",
       "        [  215,   462,  1778,  3380],\n",
       "        [  207,   199,   969, 14947]]),\n",
       " 'model': <lightgbm.basic.Booster at 0x7fb9c4e46f28>,\n",
       " 'best_params': {'colsample_bytree': 0.5838279335270615,\n",
       "  'learning_rate': 0.06754162601628992,\n",
       "  'min_child_weight': 9.612941659095199,\n",
       "  'num_boost_round': 302,\n",
       "  'num_leaves': 224,\n",
       "  'reg_alpha': 0.06187478435550153,\n",
       "  'reg_lambda': 0.07577877965357216,\n",
       "  'subsample': 0.6243898455966228,\n",
       "  'verbose': -1,\n",
       "  'num_class': 4,\n",
       "  'objective': 'multiclass'},\n",
       " 'running_time': 223.66}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnames = [str(rf).replace('../results/', '') for rf in results_fnames]\n",
    "rnames = [str(rf).replace('_results_unb.p', '') for rf in rnames]\n",
    "\n",
    "# let's have a look to one of the result files\n",
    "pickle.load(open(\"../results/nltk_tok_reviews_bigram_tfidf_results_unb.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['acc', 'f1', 'prec', 'running_time']\n",
    "resd = [] \n",
    "for rn in rnames:\n",
    "    res = pickle.load(open(\"../results/\" + rn + \"_results_unb.p\", \"rb\"))\n",
    "    res = {k:v for k,v in res.items() if k in keep}\n",
    "    res['model_name'] = rn\n",
    "    resd.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(resd)\n",
    "res_df = res_df[['model_name', 'acc', 'f1', 'prec', 'running_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>running_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spacy_tok_reviews_tfidf</td>\n",
       "      <td>0.687590</td>\n",
       "      <td>0.661757</td>\n",
       "      <td>0.654098</td>\n",
       "      <td>217.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spacy_tok_reviews_bigram_tfidf</td>\n",
       "      <td>0.687554</td>\n",
       "      <td>0.661738</td>\n",
       "      <td>0.653948</td>\n",
       "      <td>239.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nltk_tok_reviews_bigram_tfidf</td>\n",
       "      <td>0.681740</td>\n",
       "      <td>0.652874</td>\n",
       "      <td>0.645495</td>\n",
       "      <td>223.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nltk_tok_reviews_tfidf</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>0.651180</td>\n",
       "      <td>0.643643</td>\n",
       "      <td>186.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spacy_tok_reviews_lda_50</td>\n",
       "      <td>0.623959</td>\n",
       "      <td>0.562297</td>\n",
       "      <td>0.555568</td>\n",
       "      <td>25.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spacy_tok_reviews_ensemb_20</td>\n",
       "      <td>0.618145</td>\n",
       "      <td>0.555991</td>\n",
       "      <td>0.548623</td>\n",
       "      <td>28.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nltk_tok_reviews_ensemb_20</td>\n",
       "      <td>0.620334</td>\n",
       "      <td>0.555944</td>\n",
       "      <td>0.548857</td>\n",
       "      <td>29.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spacy_tok_reviews_bigram_ensemb_20</td>\n",
       "      <td>0.616925</td>\n",
       "      <td>0.553605</td>\n",
       "      <td>0.546329</td>\n",
       "      <td>29.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spacy_tok_reviews_bigram_lda_50</td>\n",
       "      <td>0.618145</td>\n",
       "      <td>0.552436</td>\n",
       "      <td>0.547555</td>\n",
       "      <td>28.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nltk_tok_reviews_bigram_ensemb_20</td>\n",
       "      <td>0.612116</td>\n",
       "      <td>0.543171</td>\n",
       "      <td>0.536745</td>\n",
       "      <td>30.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nltk_tok_reviews_lda_50</td>\n",
       "      <td>0.615310</td>\n",
       "      <td>0.539885</td>\n",
       "      <td>0.533260</td>\n",
       "      <td>26.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nltk_tok_reviews_bigram_lda_50</td>\n",
       "      <td>0.614485</td>\n",
       "      <td>0.538739</td>\n",
       "      <td>0.533699</td>\n",
       "      <td>25.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spacy_tok_reviews_bigram_lda_20</td>\n",
       "      <td>0.608419</td>\n",
       "      <td>0.536403</td>\n",
       "      <td>0.526926</td>\n",
       "      <td>27.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spacy_tok_reviews_lda_20</td>\n",
       "      <td>0.606338</td>\n",
       "      <td>0.533827</td>\n",
       "      <td>0.525180</td>\n",
       "      <td>19.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nltk_tok_reviews_lda_20</td>\n",
       "      <td>0.609568</td>\n",
       "      <td>0.533636</td>\n",
       "      <td>0.527952</td>\n",
       "      <td>23.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nltk_tok_reviews_bigram_lda_20</td>\n",
       "      <td>0.608563</td>\n",
       "      <td>0.527086</td>\n",
       "      <td>0.525355</td>\n",
       "      <td>24.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model_name       acc        f1      prec  \\\n",
       "14             spacy_tok_reviews_tfidf  0.687590  0.661757  0.654098   \n",
       "5       spacy_tok_reviews_bigram_tfidf  0.687554  0.661738  0.653948   \n",
       "0        nltk_tok_reviews_bigram_tfidf  0.681740  0.652874  0.645495   \n",
       "10              nltk_tok_reviews_tfidf  0.680556  0.651180  0.643643   \n",
       "3             spacy_tok_reviews_lda_50  0.623959  0.562297  0.555568   \n",
       "2          spacy_tok_reviews_ensemb_20  0.618145  0.555991  0.548623   \n",
       "1           nltk_tok_reviews_ensemb_20  0.620334  0.555944  0.548857   \n",
       "8   spacy_tok_reviews_bigram_ensemb_20  0.616925  0.553605  0.546329   \n",
       "12     spacy_tok_reviews_bigram_lda_50  0.618145  0.552436  0.547555   \n",
       "11   nltk_tok_reviews_bigram_ensemb_20  0.612116  0.543171  0.536745   \n",
       "15             nltk_tok_reviews_lda_50  0.615310  0.539885  0.533260   \n",
       "6       nltk_tok_reviews_bigram_lda_50  0.614485  0.538739  0.533699   \n",
       "4      spacy_tok_reviews_bigram_lda_20  0.608419  0.536403  0.526926   \n",
       "7             spacy_tok_reviews_lda_20  0.606338  0.533827  0.525180   \n",
       "13             nltk_tok_reviews_lda_20  0.609568  0.533636  0.527952   \n",
       "9       nltk_tok_reviews_bigram_lda_20  0.608563  0.527086  0.525355   \n",
       "\n",
       "    running_time  \n",
       "14        217.83  \n",
       "5         239.55  \n",
       "0         223.66  \n",
       "10        186.04  \n",
       "3          25.02  \n",
       "2          28.29  \n",
       "1          29.14  \n",
       "8          29.38  \n",
       "12         28.04  \n",
       "11         30.97  \n",
       "15         26.27  \n",
       "6          25.87  \n",
       "4          27.04  \n",
       "7          19.57  \n",
       "13         23.44  \n",
       "9          24.24  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.sort_values('f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of interesting results to discuss from this table. \n",
    "\n",
    "In the first place, we see that the best results (by far) are obtained when using tf-idf (let me add that these types of results, i.e. the best solution being the simplest or most straightforward, are quite common in ML). The main two drawbacks of this technique is that, as soon as your vocabulary is large the (sparse) feature matrix is going to be gigantic, and that due to that data-size \"*issue*\", the algorithm is relatively slow compared to other solutions. \n",
    "\n",
    "For example, the last column in the dataset shows the running time per iteration. In the case of tf-idf related experiments, I have used 50 iterations without cross validation, while for the remaining ones I have used cross-validations and 100 iterations during optimization. \n",
    "\n",
    "We can see that in the case of tf-idf is **notably slower** than when using topic modeling (note that a direct comparison is not adequate since I did not use cross-validation for tf-idf). \n",
    "\n",
    "Nonetheless, let's reflect for a bit on the `LightGBM`'s gun-power. I have used a vocabulary size of 20000 words and the dataset contains 278,677 reviews. Therefore, the tf-idf feature matrix is 278,677 x 20,000. Still, `LightGBM` manages to fit that in less than 4 min ðŸ˜±.\n",
    "\n",
    "Before we discuss some other interesing aspects, let's see how long it took to run *all* experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the 16 experiments took: 28.647 h\n"
     ]
    }
   ],
   "source": [
    "print('Running the 16 experiments took: {} h'.format(\n",
    "    round(sum(np.array([50]*4 + [100]*12) * res_df.running_time.values)/3600, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit more than a day of c5.4xlarge time ðŸ™‚.\n",
    "\n",
    "A second interesting aspect is that, in terms of classifying the documents (or predicting their score), my `Spacy` tokenization seems to work a bit better than my `nltk` one. This is simply because the later is a bit more \"aggressive\". In addition, interestingly, adding bigrams does not seem to make any difference. As we saw in previous notebooks, a better preprocessing step is possible. In the \"*real world*\" this would be another venue to explore to improve the success metrics. \n",
    "\n",
    "Moving on, it is perhaps a bit dissapointing how distant are the topic modeling techniques from the tf-idf results. It is true that we loose quite a bit of information by reducing the dimensionality to 20 or 50, but still, one would expect that the topics would have captured more information (the drop in f-score is $\\sim$10%). One might be tempted to use a higher number of topics (100 or 150). I will leave that for the reader to try...Spoiler alert, it does not improve that much (at least when using 100 topics).\n",
    "\n",
    "Still focusing on the topic modeling techniques, we see that the topic ensemble technique in `EnStop` does indeed outperform the LDA technique in `sklearn` when using the same number of topics (remember that due to memory issues related to the fact that `UMAP` does not accept sparse matrices we can only use 20 topics when using `EnStop`). I think this result highlights the potential of the `EnStop` package and I just wish they add the ability to deal with sparse matrices in the near future. \n",
    "\n",
    "So, in summary, I have spend 28.6 hours and run 16 experiments and the best accuracy and f1-score obtained are 0.688 and 0.662 respectively, but the question is of course...**is this good at all?**\n",
    "\n",
    "Let's see which results one would obtain by just doing a random guess given the observed distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class_counts = [(k,v) for k,v in Counter(dtrain.y).items()]\n",
    "class_counts = sorted(class_counts, key = lambda x: x[0])\n",
    "class_prob   = [c[1]/dtrain.y.shape[0] for c in class_counts] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40921285205422925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accs, fs = [],[]\n",
    "for _ in range(100):\n",
    "    np.random.choice(4, dtrain.y.shape[0], p=class_prob)\n",
    "    accs.append(accuracy_score(dtrain.y, random_guess))\n",
    "\n",
    "print(np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, \"thankfully\", we are doing notably better than just a random guess given the observed distributions, phew!\n",
    "\n",
    "Ok, so there are only a couple of things I want to comment here. \n",
    "\n",
    "My first comment is related to the way I have faced the problem, i.e. as a multiclass classification problem. In reality, there is nothing that prevents us from facing this problem as a regression and compare results. I will again leave that to the reader. \n",
    "\n",
    "Secondly, I have also included code to run the classification using the [Focal Loss](https://arxiv.org/abs/1708.02002). The dataset is \"mildly\" imbalanced, so it might be worth it to give it a go. I will do this in a future visit or simply leave it to the reader.\n",
    "\n",
    "For example, just run this: \n",
    "\n",
    "```python\n",
    "python score.py --packg spacy --maxevals 50 --with_focal_loss --is_unbalance --save\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
