{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have grouped the reviews into 4 classes and re-labeled them to range from [0-4), we proceed as with any other ML problme, doing some preprocessing. \n",
    "\n",
    "To tha aim I am going to use all your usual suspects, imported below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import en_core_web_sm\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data processing will be done with the following functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(doc):\n",
    "    return [t for t in simple_preprocess(doc, min_len=2) if t not in STOP_WORDS]\n",
    "\n",
    "class NLTKLemmaTokenizer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.lemmatizer.lemmatize(t, pos=\"v\") for t in simple_tokenizer(doc)]\n",
    "\n",
    "\n",
    "class SpacyLemmaTokenizer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tok = spacy.blank('en', disable=[\"parser\",\"tagger\",\"ner\"])\n",
    "\n",
    "    @staticmethod\n",
    "    def condition(t, min_len=2):\n",
    "        return not (t.is_punct | t.is_space | (t.lemma_ != '-PRON-') | len(t)<=min_len |\n",
    "            t.is_stop |  t.is_digit)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [t.lemma_.lower() for t in self.tok(doc) if self.condition(t)]\n",
    "\n",
    "\n",
    "class Bigram(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.phraser = Phraser\n",
    "\n",
    "    @staticmethod\n",
    "    def append_bigram(doc, phrases_model):\n",
    "        doc += [t for t in phrases_model[doc] if '_' in t]\n",
    "        return doc\n",
    "\n",
    "    def __call__(self, docs):\n",
    "        phrases = Phrases(docs,min_count=10)\n",
    "        bigram = self.phraser(phrases)\n",
    "        p = Pool(cores)\n",
    "        docs = p.starmap(self.append_bigram, zip(docs, [bigram]*len(docs)))\n",
    "        pool.close()\n",
    "        return docs\n",
    "\n",
    "\n",
    "def count_nouns(tokens):\n",
    "    return sum([t.pos_ is 'NOUN' for t in tokens])/len(tokens)\n",
    "\n",
    "\n",
    "def count_adjectives(tokens):\n",
    "    return sum([t.pos_ is 'ADJ' for t in tokens])/len(tokens)\n",
    "\n",
    "\n",
    "def count_adverbs(tokens):\n",
    "    return sum([t.pos_ is 'ADV' for t in tokens])/len(tokens)\n",
    "\n",
    "\n",
    "def count_verbs(tokens):\n",
    "    return sum([t.pos_ is 'VERB' for t in tokens])/len(tokens)\n",
    "\n",
    "\n",
    "def sentence_metric(tokens):\n",
    "    slen = [len(s) for s in tokens.sents]\n",
    "    metrics = np.array([np.mean(slen), np.median(slen), np.min(slen), np.max(slen)])/len(tokens)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def xtra_features(doc):\n",
    "    tokens = nlp(doc)\n",
    "    n_nouns = count_nouns(tokens)\n",
    "    n_adj   = count_adjectives(tokens)\n",
    "    n_adv   = count_adverbs(tokens)\n",
    "    n_verb  = count_verbs(tokens)\n",
    "    sent_m  = sentence_metric(tokens)\n",
    "    return [n_nouns, n_adj, n_adv, n_verb] + list(sent_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `preprocessing.py` script I have run the `LemmaTokenizer` using `Spacy` and `nltk`, with and without Bigrams. Let me illustrate the use here in the case of `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../../datasets/amazon_reviews\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH/'reviews_Clothing_Shoes_and_Jewelry.csv')\n",
    "df = df[~df.reviewText.isna()].sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "reviews = df.reviewText.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tok  = NLTKLemmaTokenizer()\n",
    "\n",
    "# Running the tokenizer in parallel\n",
    "pool = Pool(cores)\n",
    "nltk_docs  = pool.map(nltk_tok, reviews)\n",
    "pool.close()\n",
    "\n",
    "# Computing the Bigrams\n",
    "nltk_pdocs  = Bigram()(nltk_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got these earrings for my 15 year old granddaughter for Christmas.  She told me she liked jewelry that was wings.  They look like something she will like and wear.  The price was reasonable.\n"
     ]
    }
   ],
   "source": [
    "print(reviews[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'earrings', 'year', 'old', 'granddaughter', 'christmas', 'tell', 'like', 'jewelry', 'wing', 'look', 'like', 'like', 'wear', 'price', 'reasonable']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_docs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'earrings', 'year', 'old', 'granddaughter', 'christmas', 'tell', 'like', 'jewelry', 'wing', 'look', 'like', 'like', 'wear', 'price', 'reasonable', 'year_old', 'granddaughter_christmas', 'price_reasonable']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_pdocs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight one can see that perhaps a better preprocessing would be possible, for example, to break words like \"granddaughter\". For now, I will move on. In the `preprocessing.py` file you will find more code related to `Spacy` and computing what I have referred above as `xtra_features` (counts of number of nouns, adjectives, verbs, etc...)\n",
    "\n",
    "All the results are saved to disk and we are ready to extract the features that will be used for text classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
