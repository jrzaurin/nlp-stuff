{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build some sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from gensim.utils import simple_preprocess\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpus = os.cpu_count()\n",
    "tok = spacy.blank('en', disable=[\"parser\",\"tagger\",\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')\n",
    "df = pickle.load(open(DATA_PATH/'df_reviews_text.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start by coding a number of little helpers to clean the sentences so that we can average the word-vectors of the words forming those sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sents(sents):\n",
    "    nsents = []\n",
    "    for s in sents: nsents.append(' '.join([t.norm_ for t in tok.tokenizer(s)]))\n",
    "    return nsents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a great tutu and at a really great price.',\n",
       " \"It doesn't look cheap at all.\",\n",
       " \"I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly.\",\n",
       " 'A++']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a great tutu and at a really great price .',\n",
       " 'it does not look cheap at all .',\n",
       " 'i am so glad i looked on amazon and found such an affordable tutu that is not made poorly .',\n",
       " 'a++']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_sents(df.review_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following ones are self explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_non_alpha(sents):\n",
    "    return [re.sub(\"[^a-zA-Z]\", \" \", s).strip() for s in sents]\n",
    "\n",
    "\n",
    "def rm_single_chars(sents):\n",
    "    return [s for s in sents if len(s)>1]\n",
    "\n",
    "\n",
    "def rm_useless_spaces(sents):\n",
    "    return [re.sub(' {2,}', ' ', s) for s in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will process the sentences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sents(df, coln='processed_sents'):\n",
    "    df[coln] = df['review_sents'].apply(lambda x: normalize_sents(x))\n",
    "    df[coln] = df[coln].apply(lambda x: rm_non_alpha(x))\n",
    "    df[coln] = df[coln].apply(lambda x: rm_single_chars(x))\n",
    "    df[coln] = df[coln].apply(lambda x: rm_useless_spaces(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this function via pandas `apply` which can be rather slow. Here is a little `helper` to make our life better/faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_apply(df, func, n_cores=n_cpus):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.86 s, sys: 1.48 s, total: 4.34 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%time df = parallel_apply(df, process_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>review_sents</th>\n",
       "      <th>processed_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I lo...</td>\n",
       "      <td>Great tutu-  not cheaply made</td>\n",
       "      <td>[This is a great tutu and at a really great price., It doesn't look cheap at all., I'm so glad I...</td>\n",
       "      <td>[this is a great tutu and at a really great price, it does not look cheap at all, i am so glad i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I bought this for my 4 yr old daughter for dance class, she wore it today for the first time and...</td>\n",
       "      <td>Very Cute!!</td>\n",
       "      <td>[I bought this for my 4 yr old daughter for dance class, she wore it today for the first time an...</td>\n",
       "      <td>[i bought this for my yr old daughter for dance class she wore it today for the first time and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What can I say... my daughters have it in orange, black, white and pink and I am thinking to buy...</td>\n",
       "      <td>I have buy more than one</td>\n",
       "      <td>[What can I say..., my daughters have it in orange, black, white and pink, and I am thinking to ...</td>\n",
       "      <td>[what can i say, my daughters have it in orange black white and pink, and i am thinking to buy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We bought several tutus at once, and they are got high reviews. Sturdy and seemingly well-made. ...</td>\n",
       "      <td>Adorable, Sturdy</td>\n",
       "      <td>[We bought several tutus at once, and they are got high reviews., Sturdy and seemingly well-made...</td>\n",
       "      <td>[we bought several tutus at once and they are got high reviews, sturdy and seemingly well made, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you Halo Heaven great product for Little Girls.  My Great Grand Daughters Love these Tutu'...</td>\n",
       "      <td>Grammy's Angels Love it</td>\n",
       "      <td>[Thank you Halo Heaven great product for Little Girls.  , My Great Grand Daughters Love these Tu...</td>\n",
       "      <td>[thank you halo heaven great product for little girls, my great grand daughters love these tutu ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            reviewText  \\\n",
       "0  This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I lo...   \n",
       "1  I bought this for my 4 yr old daughter for dance class, she wore it today for the first time and...   \n",
       "2  What can I say... my daughters have it in orange, black, white and pink and I am thinking to buy...   \n",
       "3  We bought several tutus at once, and they are got high reviews. Sturdy and seemingly well-made. ...   \n",
       "4  Thank you Halo Heaven great product for Little Girls.  My Great Grand Daughters Love these Tutu'...   \n",
       "\n",
       "                         summary  \\\n",
       "0  Great tutu-  not cheaply made   \n",
       "1                    Very Cute!!   \n",
       "2       I have buy more than one   \n",
       "3               Adorable, Sturdy   \n",
       "4        Grammy's Angels Love it   \n",
       "\n",
       "                                                                                          review_sents  \\\n",
       "0  [This is a great tutu and at a really great price., It doesn't look cheap at all., I'm so glad I...   \n",
       "1  [I bought this for my 4 yr old daughter for dance class, she wore it today for the first time an...   \n",
       "2  [What can I say..., my daughters have it in orange, black, white and pink, and I am thinking to ...   \n",
       "3  [We bought several tutus at once, and they are got high reviews., Sturdy and seemingly well-made...   \n",
       "4  [Thank you Halo Heaven great product for Little Girls.  , My Great Grand Daughters Love these Tu...   \n",
       "\n",
       "                                                                                       processed_sents  \n",
       "0  [this is a great tutu and at a really great price, it does not look cheap at all, i am so glad i...  \n",
       "1  [i bought this for my yr old daughter for dance class she wore it today for the first time and t...  \n",
       "2  [what can i say, my daughters have it in orange black white and pink, and i am thinking to buy f...  \n",
       "3  [we bought several tutus at once and they are got high reviews, sturdy and seemingly well made, ...  \n",
       "4  [thank you halo heaven great product for little girls, my great grand daughters love these tutu ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with `swifter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a460e4a7624e40b075be3040bf2f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=265728, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import swifter\n",
    "df['processed_sents'] = df['review_sents'].swifter.apply(lambda x: normalize_sents(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So...is slower. Only uses 4 cores in my Mac. Based on their repo I'd say this would be useful for larger datasets\n",
    "\n",
    "Anyway, moving onto word and sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vectors(path, fname):\n",
    "    embeddings_index = {}\n",
    "    f = open(path/fname)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def sentence_vector(sent, embeddings_index, dim=100):\n",
    "    if len(sent)>0:\n",
    "        return sum([embeddings_index.get(w, np.zeros((dim,))) for w in sent])/len(sent)\n",
    "    else:\n",
    "        return np.zeros((dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDVEC_PATH = DATA_PATH/'glove.6B'\n",
    "wordvec_fname= 'glove.6B.100d.txt'\n",
    "embeddings_index = word_vectors(WORDVEC_PATH, wordvec_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual sentences\n",
    "all_sents = [s for sents in df.processed_sents for s in sents]\n",
    "idx2sent = {k:v for k,v in enumerate(all_sents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2toksents = {}\n",
    "for i,s in idx2sent.items(): idx2toksents[i] = simple_preprocess(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'great', 'tutu', 'and', 'at', 'really', 'great', 'price']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2toksents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2vec = {}\n",
    "for i,s in idx2toksents.items(): sent2vec[i] = sentence_vector(s, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08077522,  0.40300232,  0.46199453, -0.17641734, -0.08382   ,\n",
       "       -0.1148019 , -0.22886679,  0.12189113, -0.31666443, -0.27374446,\n",
       "       -0.01265999,  0.02562739,  0.08072   ,  0.03578821,  0.03440867,\n",
       "       -0.30850387,  0.02228022,  0.04734689, -0.44706333,  0.29923317,\n",
       "        0.15157245,  0.0429249 ,  0.12130955, -0.13901   ,  0.55765444,\n",
       "        0.15392989, -0.19783   , -0.39939177,  0.07522634, -0.30169234,\n",
       "       -0.26608667,  0.41936848,  0.15411378,  0.0793588 ,  0.02022355,\n",
       "        0.26441944,  0.04570355,  0.44493055, -0.04780278, -0.24634112,\n",
       "       -0.33693856, -0.10056976,  0.16340989, -0.51877445, -0.06957345,\n",
       "        0.12563667,  0.48675224, -0.41143847, -0.04714622, -0.7283745 ,\n",
       "        0.06338234, -0.23116666,  0.22893542,  0.94118   , -0.315755  ,\n",
       "       -2.5162    , -0.2728801 , -0.07546222,  1.2681334 ,  0.26414666,\n",
       "       -0.10944656,  0.65772665, -0.4733538 ,  0.08199911,  0.57762116,\n",
       "       -0.20582278,  0.5442811 ,  0.20293446,  0.41186   , -0.10732323,\n",
       "        0.06111123, -0.27591965, -0.09801254, -0.24015707,  0.16722433,\n",
       "       -0.03763644,  0.03376834,  0.03171812, -0.69815224,  0.04209244,\n",
       "        0.47512525,  0.11167067, -0.31206557,  0.14588022, -1.1948522 ,\n",
       "       -0.12460223, -0.16078979, -0.43916303, -0.06048567, -0.4644033 ,\n",
       "       -0.07878356, -0.23283142, -0.12598357,  0.2763356 , -0.5340524 ,\n",
       "        0.08876666, -0.25662544, -0.64088064,  0.44753668,  0.51552176],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1409349"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and just like that we have 1.4 mill sentence vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
