{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the tf-idf matrix is just a matter of:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "from utils.lightgbm_optimizer import LGBOptimizer\n",
    "from pathlib import Path\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\topt = LGBOptimizer(dataset='original', save=True)\n",
    "\topt.optimize(maxevals=50)\n",
    "\n",
    "\topt = LGBOptimizer(dataset='augmented', save=True)\n",
    "\topt.optimize(maxevals=50)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't run the code here since it takes a long time. Let's have a look to the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_res = pickle.load(open(\"../results/original_results.p\", \"rb\"))\n",
    "aug_res = pickle.load(open(\"../results/augmented_results.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS WITH ORIGINAL DATASET: {'acc': 0.7054056974105953, 'f1': 0.6832450020682459, 'prec': 0.6763347945728874, 'rec': 0.7054056974105953, 'cm': array([[ 1613,   406,   215,   423],\n",
      "       [  549,   897,   864,   723],\n",
      "       [  183,   455,  2144,  3037],\n",
      "       [  137,   175,  1013, 14933]]), 'model': <lightgbm.basic.Booster object at 0x7f1f6e5c2320>, 'best_params': {'colsample_bytree': 0.7333187629999629, 'learning_rate': 0.0639446526095139, 'min_child_weight': 6.386698616449527, 'num_boost_round': 439, 'num_leaves': 236, 'reg_alpha': 0.08333569598110195, 'reg_lambda': 0.07805240800326783, 'subsample': 0.5715270609261924, 'verbose': -1, 'num_class': 4, 'objective': 'multiclass'}, 'running_time': 311.58}\n",
      "RESULTS WITH EDA AUGMENTED DATASET: {'acc': 0.7060899629056073, 'f1': 0.6847974561849195, 'prec': 0.6781136678067431, 'rec': 0.7060899629056073, 'cm': array([[ 1598,   414,   217,   428],\n",
      "       [  530,   919,   866,   718],\n",
      "       [  161,   443,  2185,  3030],\n",
      "       [  121,   175,  1058, 14904]]), 'model': <lightgbm.basic.Booster object at 0x7f1f6e5c2240>, 'best_params': {'colsample_bytree': 0.5432022623843756, 'learning_rate': 0.06561449030789117, 'min_child_weight': 1.6249908553797112, 'num_boost_round': 460, 'num_leaves': 224, 'reg_alpha': 0.07358964852544704, 'reg_lambda': 0.02790703499576671, 'subsample': 0.9136675075788329, 'verbose': -1, 'num_class': 4, 'objective': 'multiclass'}, 'running_time': 1027.66}\n"
     ]
    }
   ],
   "source": [
    "print('RESULTS WITH ORIGINAL DATASET: {}'.format(org_res))\n",
    "print('RESULTS WITH EDA AUGMENTED DATASET: {}'.format(aug_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_keys = ['acc', 'f1', 'prec', 'running_time']\n",
    "o_res = {k:v for k,v in org_res.items() if k in keep_keys}\n",
    "o_res['data'] = 'original'\n",
    "a_res = {k:v for k,v in aug_res.items() if k in keep_keys}\n",
    "a_res['data'] = 'augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>running_time</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.705406</td>\n",
       "      <td>0.683245</td>\n",
       "      <td>0.676335</td>\n",
       "      <td>311.58</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.706090</td>\n",
       "      <td>0.684797</td>\n",
       "      <td>0.678114</td>\n",
       "      <td>1027.66</td>\n",
       "      <td>augmented</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc        f1      prec  running_time       data\n",
       "0  0.705406  0.683245  0.676335        311.58   original\n",
       "0  0.706090  0.684797  0.678114       1027.66  augmented"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res = pd.concat([pd.DataFrame(o_res, index=[0]), pd.DataFrame(a_res, index=[0])])\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of points to comment here. First and of course the most important is that all metrics have improved by $\\sim$2% relative to the results in the dir `amazon_reviews_classification_without_DL` where I did not use the `fastai`'s Tokenizer. Second one can see that the difference between using and not using EDA in this dataset is negligible. To be honest I was expecting this results since EDA is more suited for DL algorithms and small datasets. And finally let me comment on the running time. In the case of the augmented dataset we are feeding to `LightGBM` a sparse matrix of nearly 0.9mil rows and 30k columns. Each full `hyperopt` iteration takes around 1000 sec, or around 15min (note that the running time includes the final fit, i.e. the 51 `LightGBM` fits, 50 `hyperopt` iterations (using train and valid) plus the final one (using train+valid and test)). So fitting 0.9mil rows and 30k columns takes around 15min. \n",
    "\n",
    "Anyway, the summary for the exercise in this repo is: **USE FASTAI TOKENIZER**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
