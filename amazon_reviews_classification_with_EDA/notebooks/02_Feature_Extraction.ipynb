{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is augmented we need to extract the features, which in this case is going to be simply using tf-idf (you might want to give it a go to just counting words (`CountVectorizer`) ). The code in this repo corresponds to the code in `feature_extraction.py`. It is possible that you run into memory issues as you run the script. This is mostly due to `fastai`'s tokenizer. If you run into those problems, you could use go to these lines:\n",
    "\n",
    "```python\n",
    "reviews = df['reviewText'].tolist()\n",
    "tok_reviews = tok.process_all(reviews)\n",
    "```\n",
    "\n",
    "and break the tokenization in chuncks. \n",
    "\n",
    "As you see, I am using `fastai`'s tokenizer applied directly to the text without any preprocessing (no lowercase, no lemmatization, no removing punctuation, nothing). This is because that tokenizer does a number of things under the hood and the more information is in the text the better. For example, they have tokens to indicate whether the next word starts with upper case, or whether some characters are repeated (and how many times) etc. All this might be relevant in terms of classifying reviews.\n",
    "\n",
    "The process of building the features (i.e. tf-idf matrix) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from fastai.text import Tokenizer\n",
    "from eda import EDA\n",
    "\n",
    "tok = Tokenizer()\n",
    "\n",
    "def extract_features(df, out_path, max_features=30000, vectorizer=None):\n",
    "\n",
    "    reviews = df['reviewText'].tolist()\n",
    "    tok_reviews = tok.process_all(reviews)\n",
    "\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features, preprocessor=lambda x: x,\n",
    "            tokenizer = lambda x: x, min_df=5)\n",
    "        X = vectorizer.fit_transform(tok_reviews)\n",
    "    else:\n",
    "        X = vectorizer.transform(tok_reviews)\n",
    "\n",
    "    featset = Bunch(X=X, y=df.overall.values)\n",
    "    pickle.dump(featset, open(out_path, 'wb'))\n",
    "\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH  = Path('../data')\n",
    "FEAT_PATH  = Path('../features')\n",
    "FTRAIN_PATH = FEAT_PATH/'train'\n",
    "FVALID_PATH = FEAT_PATH/'valid'\n",
    "FTEST_PATH  = FEAT_PATH/'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORIGINAL TEXT\n",
    "\n",
    "(the code below will take a few seconds to run on a c5.4xlarge EC2 instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building train/valid features for original dataset\n",
      "building train/test features for original dataset\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH/'train/train.csv')\n",
    "valid = pd.read_csv(DATA_PATH/'valid/valid.csv')\n",
    "test  = pd.read_csv(DATA_PATH/'test/test.csv')\n",
    "\n",
    "# we will tune parameters with the 80% train and 10% validation\n",
    "print(\"building train/valid features for original dataset\")\n",
    "vec = extract_features(train, out_path=FTRAIN_PATH/'ftrain.p')\n",
    "_ = extract_features(valid, vectorizer=vec, out_path=FVALID_PATH/'fvalid.p')\n",
    "\n",
    "# once we have tuned parameters we will train on 'train+valid' (90%) and test\n",
    "# on 'test' (10%)\n",
    "print(\"building train/test features for original dataset\")\n",
    "full_train = pd.concat([train, valid]).sample(frac=1).reset_index(drop=True)\n",
    "fvec = extract_features(full_train, out_path=FTRAIN_PATH/'fftrain.p')\n",
    "_ = extract_features(test, vectorizer=fvec, out_path=FTEST_PATH/'ftest.p')\n",
    "del (train, vec, fvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUGMENTED TEXT\n",
    "\n",
    "This will take a bit longer and depending on your machine you might run into memory issues. In fact, I actually run into those issues. To overcome the memory error I simply run the whole script (`feature_extraction.py`) until the last few lines (commented) and the run the last few lines on their own: \n",
    "\n",
    "```python\n",
    "print(\"building train/test features for augmented dataset\")\n",
    "a_full_train = (pd.read_csv(DATA_PATH/'train/a_full_train.csv', engine='python', sep=\"::\",\n",
    "    names=['overall', 'reviewText'])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True))\n",
    "a_fvec = extract_features(a_full_train, out_path=FTRAIN_PATH/'a_fftrain.p')\n",
    "del a_full_train\n",
    "_ = extract_features(test, vectorizer=a_fvec, out_path=FTEST_PATH/'a_ftest.p')\n",
    "```\n",
    "\n",
    "Also, for some reason this will not work in a notebook, but does work from the terminal. A MORE ELEGANT solution is to break reviews into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building train/valid features for augmented dataset\n"
     ]
    }
   ],
   "source": [
    "# AUGMENTED\n",
    "# Only the training set \"at the time\" must be augmented\n",
    "a_train = (pd.read_csv(DATA_PATH/'train/a_train.csv', engine='python', sep=\"::\",\n",
    "    names=['overall', 'reviewText'])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True))\n",
    "\n",
    "# we will tune parameters with the 80% train and 10% validation. At this\n",
    "# stage, the validation set should not be augmented, but we need to compute\n",
    "# the validation features with the \"augmented vectorizer\"\n",
    "print(\"building train/valid features for augmented dataset\")\n",
    "a_vec = extract_features(a_train, out_path=FTRAIN_PATH/'a_ftrain.p')\n",
    "_ = extract_features(valid, vectorizer=a_vec, out_path=FVALID_PATH/'a_fvalid.p')\n",
    "\n",
    "# once we have tuned parameters we will:\n",
    "# 1-augment 'train+valid'\n",
    "# 2-train the vectorizer on augmented dataset\n",
    "# 3-use the augmented vectorizer on 'test'\n",
    "print(\"building augmented dataset for train+valid\")\n",
    "eda = EDA(rs=False)\n",
    "full_train = list(full_train[['reviewText', 'overall']].itertuples(index=False, name=None))\n",
    "eda.augment(full_train, out_file=DATA_PATH/'train/a_full_train.csv')\n",
    "del (valid, full_train)\n",
    "\n",
    "print(\"building train/test features for augmented dataset\")\n",
    "a_full_train = (pd.read_csv(DATA_PATH/'train/a_full_train.csv', engine='python', sep=\"::\",\n",
    "    names=['overall', 'reviewText'])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True))\n",
    "a_fvec = extract_features(a_full_train, out_path=FTRAIN_PATH/'a_fftrain.p')\n",
    "del a_full_train\n",
    "_ = extract_features(test, vectorizer=a_fvec, out_path=FTEST_PATH/'a_ftest.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once the features are created, is just a matter of \"plugging them\" into LightGBM and off we go."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
