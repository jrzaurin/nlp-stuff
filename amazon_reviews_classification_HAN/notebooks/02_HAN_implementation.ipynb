{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: to run the notebooks move them to the main dir. Simply\n",
    "\n",
    "```bash\n",
    "cp notebook_name.ipynd ../\n",
    "```\n",
    "\n",
    "Once the data is prepared (see Notebook 01), let's have a look to the HAN implementation.\n",
    "\n",
    "Let's start by bringing back the figure of the network architecture and the mathematical expressions corresponding to the attention mechanism used by the authors\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"400\" src=\"figures/HAN_arch.png\">\n",
    "</p>\n",
    "\n",
    "**Word attention:**\n",
    "\n",
    "$\n",
    "u_{it} = \\text{tanh}(W_wh_{it} + b_w)\n",
    "$\n",
    "\n",
    "$\n",
    "\\alpha_{it} = \\frac{\\exp(u_{it}u_w^{\\mathsf{T}})}{\\sum_{t}\\exp(u_{it}u_w^{\\mathsf{T}})}\n",
    "$\n",
    "\n",
    "$\n",
    "s_i = \\sum_{t}\\alpha_{it}h_{it}\n",
    "$\n",
    "\n",
    "Where $u_{it}$ can be seen as a hidden representation of $h_{it}$ (the GRU ouput). The importance of a word is then measured as the similarity of $u_{it}$ with a word level\n",
    "context vector $u_{w}$, which is then normalized through a softmax function resulting in  $\\alpha_{it}$, the so called normalized importance\n",
    "weights. The sentence vector $s_i$ is the weighted sum of the word annotations based on the weights $\\alpha_{it}$. For more details please, have a look to the paper [Zichao Yang et al., 2016](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf). \n",
    "\n",
    "The same as before applies to the sentence attention mechanism but at sentence level.\n",
    "\n",
    "**Sentence attention:**\n",
    "\n",
    "$\n",
    "u_{i} = \\text{tanh}(W_sh_i + b_s)\n",
    "$\n",
    "\n",
    "$\n",
    "\\alpha_{i} = \\frac{\\exp(u_iu_s^{\\mathsf{T}})}{\\sum_{i}\\exp(u_{i}u_s^{\\mathsf{T}})}\n",
    "$\n",
    "\n",
    "$\n",
    "v = \\sum_{i}\\alpha_{i}h_{i}\n",
    "$\n",
    "\n",
    "In a simplified way, the flow of the data is: Word Embeddings $\\rightarrow$ GRU $\\rightarrow$ Word Attention $\\rightarrow$ GRU $\\rightarrow$ Sentence Attention $\\rightarrow$ FC + Softmax\n",
    "\n",
    "Let's build the pieces one by one, starting with the attention mechanism. I have kept the names of the variables as close as possible to the notation of the paper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # The first expression in the attention mechanism is simply a linear layer that receives \n",
    "        # the output of the Word-GRU referred here as 'inp' and h_{it} in the paper\n",
    "        u = torch.tanh_(self.attn(inp))\n",
    "        #Â The second expression is...the same but without bias, wrapped up in a Softmax function\n",
    "        a = F.softmax(self.contx(u), dim=1)\n",
    "        #Â And finally, an element-wise multiplication taking advantage of Pytorch's broadcasting abilities \n",
    "        s = (a * inp).sum(1)\n",
    "        # we will also return the normalized importance weights\n",
    "        return a.permute(0, 2, 1), s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that one could easily implement Attention without the context vector $u_w$ as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, seq_len):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.weight = nn.Parameter(nn.init.kaiming_normal_(torch.Tensor(hidden_dim, 1)))\n",
    "        self.bias = nn.Parameter(torch.zeros(seq_len))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # 1. Matrix Multiplication\n",
    "        x = inp.contiguous().view(-1, self.hidden_dim)\n",
    "        u = torch.tanh_(torch.mm(x, self.weight).view(-1, self.seq_len) + self.bias)\n",
    "        # 2. Softmax on 'u_{it}' directly\n",
    "        a = F.softmax(u, dim=1)\n",
    "        # 3. Braodcasting and out\n",
    "        s = (inp * torch.unsqueeze(a, 2)).sum(1)\n",
    "        return a, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so Attention is as easy as a few lines of code. Let's now use these functions to implement the **Word Attention Net** and the **Sentence Attention Net**. \n",
    "\n",
    "Word Attention Net looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAttnNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        hidden_dim=32,\n",
    "        padding_idx=1,\n",
    "        embed_dim=50,\n",
    "        embedding_matrix=None,\n",
    "    ):\n",
    "        super(WordAttnNet, self).__init__()\n",
    "\n",
    "        if isinstance(embedding_matrix, np.ndarray):\n",
    "            self.word_embed = nn.Embedding(\n",
    "                vocab_size, embedding_matrix.shape[1], padding_idx=padding_idx\n",
    "            )\n",
    "            self.word_embed.weight = nn.Parameter(torch.Tensor(embedding_matrix))\n",
    "            embed_dim = embedding_matrix.shape[1]\n",
    "        else:\n",
    "            self.word_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.word_attn = AttentionWithContext(hidden_dim * 2)\n",
    "\n",
    "    def forward(self, X, h_n):\n",
    "        embed = self.word_embed(X.long())\n",
    "        h_t, h_n = self.rnn(embed, h_n)\n",
    "        a, s = self.word_attn(h_t)\n",
    "        return a, s.unsqueeze(1), h_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `WordAttnNet` class at the `models` module has a few more rings and bells that I will comment in a separate notebook. However, the main parts are all contained in the cell above, so let's comment on them: \n",
    "\n",
    "1. Word Embeddings: we allow the user to pass some pre-trained word embeddings. If not, then we simply initialize them with Pytorch defaults (random). \n",
    "2. Word GRU: `batch_first=True`. I am a maniac and I want my batches first\n",
    "3. Word Attention (with context)\n",
    "\n",
    "Let's just manually run the forward pass perhaps understand better what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 16\n",
    "maxlen_sent = 20 \n",
    "hidden_dim  = 32    \n",
    "embed_dim   = 100    \n",
    "vocab_size  = 1000\n",
    "padding_idx = 1\n",
    "\n",
    "# net\n",
    "word_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "rnn = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "attn = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
    "contx = nn.Linear(hidden_dim*2, 1, bias=False)\n",
    "\n",
    "# inputs\n",
    "X = torch.from_numpy(np.random.choice(vocab_size, (bsz, maxlen_sent)))\n",
    "h_n = torch.zeros((2, bsz, hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` will be, for each review in the batch, one sentence at a time. This is because `word_attn` will run in a loop so that we can initialize the hidden state of the next sentence with the last hidden state of the previous one. We could call it *\"sentence stateful\"*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Word Embeddings\n",
    "# (bsz, maxlen_sent, embed_dim)\n",
    "embed = word_embed(X)\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sentence of each review in the batch is now represented by a sequence of 20 words with 100 dim embeddings. Now we pass these embeddings to the GRU which will return a new encoding of dim `hidden_dim * 2`, since the GRU is bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Â 2. GRU (h_t is normally referred as output\n",
    "h_t, h_n = rnn(embed, h_n)\n",
    "# (bsz, seq_len, hidden_dim*2)\n",
    "h_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism will *\"collapse\"* each sentence into a `hidden_dim * 2` tensor that is the result of a weighted average using the \"importance weights\" $\\alpha_{it}$. Therefore, the resulting output will be of dim `(bzs, hidden_dim * 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20, 64]) torch.Size([16, 20, 1])\n"
     ]
    }
   ],
   "source": [
    "# 3. Attention\n",
    "u = torch.tanh_(attn(h_t))\n",
    "a = F.softmax(contx(u), dim=1)\n",
    "print(h_t.shape, a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And not we are going to multiply broadcasting along the last dim of `h_t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20, 64])\n",
      "torch.Size([16, 64])\n"
     ]
    }
   ],
   "source": [
    "s = (a * h_t)\n",
    "#Â RNN outputs scaled by their importance weights\n",
    "print(s.shape)\n",
    "#Â Sum along the seq dim so we end up with a representation per document/review\n",
    "s = s.sum(1)\n",
    "print(s.shape)\n",
    "# Because this will be stack for all sentences, we do the `.unsqueeze(1)`\n",
    "print(s.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have, per review, a 64 dim tensor representation. Let's have a look to what is an implementation of the Sentence Attention Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentAttnNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, word_hidden_dim=32, sent_hidden_dim=32, padding_idx=1\n",
    "    ):\n",
    "        super(SentAttnNet, self).__init__()\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            word_hidden_dim * 2, sent_hidden_dim, bidirectional=True, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.sent_attn = AttentionWithContext(sent_hidden_dim * 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h_t, h_n = self.rnn(X)\n",
    "        a, v = self.sent_attn(h_t)\n",
    "        return a.permute(0,2,1), v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see is rather easy. The same comment I made before for `WordAttnNet` applies here. The class at the `models` module has a some additional functionalities that I will comment in a separate notebook, but for now, these will be enough. I don't think this needs much explanation, does it? \n",
    "\n",
    "The forward pass will receive a tensor of dim `(bsz, review_len, word_hidden_dim*2)` and it will return a tensor of dim `(bsz, sent_hidden_dim*2)`. \n",
    "\n",
    "Ok, so, we are ready to implement **Hierarchical Attention Networks** as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierAttnNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        maxlen_sent,\n",
    "        maxlen_doc,\n",
    "        word_hidden_dim=32,\n",
    "        sent_hidden_dim=32,\n",
    "        padding_idx=1,\n",
    "        embed_dim=50,\n",
    "        embedding_matrix=None,\n",
    "        num_class=4,\n",
    "    ):\n",
    "        super(HierAttnNet, self).__init__()\n",
    "\n",
    "        self.word_hidden_dim = word_hidden_dim\n",
    "\n",
    "        self.wordattnnet = WordAttnNet(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_dim=word_hidden_dim,\n",
    "            padding_idx=padding_idx,\n",
    "            embed_dim=embed_dim,\n",
    "            embedding_matrix=embedding_matrix,\n",
    "        )\n",
    "\n",
    "        self.sentattnnet = SentAttnNet(\n",
    "            word_hidden_dim=word_hidden_dim,\n",
    "            sent_hidden_dim=sent_hidden_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(sent_hidden_dim * 2, num_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = X.permute(1, 0, 2)\n",
    "        word_h_n = nn.init.zeros_(torch.Tensor(2, X.shape[0], self.word_hidden_dim))\n",
    "        if use_cuda:\n",
    "            word_h_n = word_h_n.cuda()\n",
    "        # alpha and s Tensor Lists\n",
    "        word_a_list, word_s_list = [], []\n",
    "        for sent in x:\n",
    "            word_a, word_s, word_h_n = self.wordattnnet(sent, word_h_n)\n",
    "            word_a_list.append(word_a)\n",
    "            word_s_list.append(word_s)\n",
    "        # Importance attention weights per word in sentence\n",
    "        self.sent_a = torch.cat(word_a_list, 1)\n",
    "        #Â Sentences representation\n",
    "        sent_s = torch.cat(word_s_list, 1)\n",
    "        # Importance attention weights per sentence in doc and document representation\n",
    "        self.doc_a, doc_s = self.sentattnnet(sent_s)\n",
    "        return self.fc(doc_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again see what happens in the forward pass step by step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_sent = 20\n",
    "maxlen_doc = 5\n",
    "num_class = 4\n",
    "word_hidden_dim = 32\n",
    "sent_hidden_dim = 32\n",
    "\n",
    "wordattnnet = WordAttnNet(vocab_size, hidden_dim, padding_idx, embed_dim, embedding_matrix=None)\n",
    "sentattnnet = SentAttnNet(word_hidden_dim, sent_hidden_dim, padding_idx)\n",
    "fc = nn.Linear(sent_hidden_dim * 2, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(np.random.choice(vocab_size, (bsz, maxlen_doc, maxlen_sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first permute/transpose axis so we have the input in the form `(maxlen_doc, bsz, maxlen_sent)`. This is because we are going to loop through sentences per document in each batch. Note that if you don't care about the stateful nature of the sentences in the document, you could just applied attention *\"a la TimeDistributed\"*. This is, reshaping the input along the sequence dimension, apply `wordattnnet` to the resulting tensor and reshape back to the original form before applying `sentattnnet`. However, you should care about the fact that sentences naturally follow each other, so let's do it right ðŸ™‚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 20])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X.permute(1, 0, 2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Word RNN hidden state\n",
    "word_h_n = nn.init.zeros_(torch.Tensor(2, X.shape[0], word_hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 20])\n",
      "torch.Size([16, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "# Loop through sentences:\n",
    "word_a_list, word_s_list = [], []\n",
    "for sent in x:\n",
    "    word_a, word_s, word_h_n = wordattnnet(sent, word_h_n)\n",
    "    word_a_list.append(word_a)\n",
    "    word_s_list.append(word_s)\n",
    "# Importance attention weights per word in sentence\n",
    "sent_a = torch.cat(word_a_list, 1)\n",
    "#Â Sentences representation\n",
    "sent_s = torch.cat(word_s_list, 1)\n",
    "#Â (bsz, maxlen_doc, maxlen_sent)\n",
    "print(sent_a.shape)\n",
    "#Â (bsz, maxlen_doc, hidden_dim*2)\n",
    "print(sent_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 1])\n",
      "torch.Size([16, 64])\n"
     ]
    }
   ],
   "source": [
    "doc_a, doc_s = sentattnnet(sent_s)\n",
    "#Â (bsz, maxlen_doc, 1). One could .squeeze(2) but for now I will leave it as it is\n",
    "print(doc_a.shape)\n",
    "#Â (bsz, hidden_dim*2)\n",
    "print(doc_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and predictions (without softmax, since the loss we will be using, `F.cross_entropy` already applies the logSoftmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = fc(doc_s)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
