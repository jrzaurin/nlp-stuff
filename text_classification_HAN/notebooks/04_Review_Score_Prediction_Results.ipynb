{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have run a number of experiments which are detailed in the file `run_experiments.sh`. \n",
    "\n",
    "Here I will discuss them with some detail. Note that, in total, I have **manually** run 40 experiments for the Pytorch implementation and 19 for the mxnet implementation. Of course, this is **NOT** the best way to find the best set of parameters. Ideally, one would wrap up the process into an objective function and use [`hyperopt`](http://hyperopt.github.io/hyperopt/), leaving the hyperparamter search running for a few days. \n",
    "\n",
    "I intend to include a script in the repo to do precisely that. For now, I just wanted to get an idea of the potential of HANs to predict amazon reviews score relative to other techniques I have used in this repo. \n",
    "\n",
    "Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path(\"results/\")\n",
    "pytorch_fname = \"results_df.csv\"\n",
    "mxnet_fname = \"mx_results_df.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables below show the evaluation metrics obtained **on the test dataset**, along with the best epoch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelname</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>best_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>han_lr_0.0005_wdc_0.05_bsz_512_whd_128_shd_128_emb_300_drp_0.2_sch_cycliclr_cycl_4_lrp_no_pre_no</td>\n",
       "      <td>0.676955</td>\n",
       "      <td>0.722332</td>\n",
       "      <td>0.709809</td>\n",
       "      <td>0.703325</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>han_lr_0.001_wdc_0.05_bsz_512_whd_128_shd_128_emb_300_drp_0.2_sch_reducelronplateau_cycl_no_lrp_...</td>\n",
       "      <td>0.672373</td>\n",
       "      <td>0.720424</td>\n",
       "      <td>0.707064</td>\n",
       "      <td>0.700494</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_64_whd_64_shd_64_emb_300_drp_0.2_sch_reducelronplateau_cycl_no_lrp_2_p...</td>\n",
       "      <td>0.677667</td>\n",
       "      <td>0.720460</td>\n",
       "      <td>0.706726</td>\n",
       "      <td>0.701034</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_128_whd_64_shd_64_emb_300_drp_0.2_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.679628</td>\n",
       "      <td>0.721108</td>\n",
       "      <td>0.706106</td>\n",
       "      <td>0.700320</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>han_lr_0.0005_wdc_0.05_bsz_512_whd_128_shd_128_emb_300_drp_0.2_sch_cycliclr_cycl_2_lrp_no_pre_no</td>\n",
       "      <td>0.684018</td>\n",
       "      <td>0.718407</td>\n",
       "      <td>0.704798</td>\n",
       "      <td>0.698459</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_64_whd_64_shd_64_emb_300_drp_0.2_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.681003</td>\n",
       "      <td>0.716930</td>\n",
       "      <td>0.703556</td>\n",
       "      <td>0.698334</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_512_whd_64_shd_64_emb_300_drp_0.2_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.684646</td>\n",
       "      <td>0.718155</td>\n",
       "      <td>0.702430</td>\n",
       "      <td>0.696197</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.702976</td>\n",
       "      <td>0.708827</td>\n",
       "      <td>0.698351</td>\n",
       "      <td>0.693251</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_128_whd_64_shd_64_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.693445</td>\n",
       "      <td>0.711528</td>\n",
       "      <td>0.698188</td>\n",
       "      <td>0.694117</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>han_lr_0.001_wdc_0.05_bsz_128_whd_32_shd_32_emb_100_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.696123</td>\n",
       "      <td>0.712897</td>\n",
       "      <td>0.698097</td>\n",
       "      <td>0.692957</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>han_lr_0.0005_wdc_0.01_bsz_64_whd_64_shd_64_emb_300_drp_0.2_sch_cycliclr_cycl_1_lrp_no_pre_no</td>\n",
       "      <td>0.695813</td>\n",
       "      <td>0.712392</td>\n",
       "      <td>0.697468</td>\n",
       "      <td>0.690748</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>han_lr_0.001_wdc_0.05_bsz_128_whd_32_shd_32_emb_300_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.695857</td>\n",
       "      <td>0.712176</td>\n",
       "      <td>0.697281</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_512_whd_128_shd_128_emb_300_drp_0.5_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.690965</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.697224</td>\n",
       "      <td>0.689717</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>han_lr_0.0005_wdc_0.01_bsz_64_whd_64_shd_64_emb_300_drp_0.2_sch_cycliclr_cycl_4_lrp_no_pre_no</td>\n",
       "      <td>0.689646</td>\n",
       "      <td>0.715201</td>\n",
       "      <td>0.697202</td>\n",
       "      <td>0.690485</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>han_lr_0.0005_wdc_0.05_bsz_512_whd_128_shd_128_emb_300_drp_0.2_sch_cycliclr_cycl_1_lrp_no_pre_no</td>\n",
       "      <td>0.686094</td>\n",
       "      <td>0.716894</td>\n",
       "      <td>0.697059</td>\n",
       "      <td>0.691581</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_128_whd_128_shd_128_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.699591</td>\n",
       "      <td>0.710880</td>\n",
       "      <td>0.696889</td>\n",
       "      <td>0.693631</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>han_lr_0.001_wdc_0.05_bsz_128_whd_32_shd_32_emb_200_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.697555</td>\n",
       "      <td>0.709691</td>\n",
       "      <td>0.696652</td>\n",
       "      <td>0.690477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_32_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.692085</td>\n",
       "      <td>0.713797</td>\n",
       "      <td>0.695815</td>\n",
       "      <td>0.690835</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_128_whd_128_shd_128_emb_300_drp_0.5_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.691776</td>\n",
       "      <td>0.710160</td>\n",
       "      <td>0.695556</td>\n",
       "      <td>0.688225</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_64_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.696436</td>\n",
       "      <td>0.708575</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>0.688867</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.702082</td>\n",
       "      <td>0.707531</td>\n",
       "      <td>0.694211</td>\n",
       "      <td>0.689066</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>han_lr_0.001_wdc_0.05_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.696131</td>\n",
       "      <td>0.707639</td>\n",
       "      <td>0.694087</td>\n",
       "      <td>0.688432</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>han_lr_0.0005_wdc_0.01_bsz_64_whd_64_shd_64_emb_300_drp_0.2_sch_cycliclr_cycl_2_lrp_no_pre_no</td>\n",
       "      <td>0.702785</td>\n",
       "      <td>0.705658</td>\n",
       "      <td>0.692895</td>\n",
       "      <td>0.686547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_64_whd_128_shd_128_emb_300_drp_0.5_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.696708</td>\n",
       "      <td>0.711132</td>\n",
       "      <td>0.691840</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_128_whd_16_shd_16_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.711794</td>\n",
       "      <td>0.706774</td>\n",
       "      <td>0.690902</td>\n",
       "      <td>0.683959</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_256_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.703306</td>\n",
       "      <td>0.711348</td>\n",
       "      <td>0.689721</td>\n",
       "      <td>0.683954</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>han_lr_0.001_wdc_0.01_bsz_512_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no</td>\n",
       "      <td>0.702623</td>\n",
       "      <td>0.709115</td>\n",
       "      <td>0.689221</td>\n",
       "      <td>0.682835</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rnn_lr_0.0005_wdc_0.05_bsz_512_nl_3_hd_128_emb_300_rdrp_0.2_drp_0.2_sch_cycliclr_cycl_4_lrp_no_a...</td>\n",
       "      <td>0.799264</td>\n",
       "      <td>0.671120</td>\n",
       "      <td>0.642041</td>\n",
       "      <td>0.635197</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_512_nl_3_hd_128_emb_300_rdrp_0.2_drp_0.2_sch_no_cycl_no_lrp_no_att_no_...</td>\n",
       "      <td>0.810204</td>\n",
       "      <td>0.665250</td>\n",
       "      <td>0.636496</td>\n",
       "      <td>0.627285</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.05_bsz_512_nl_3_hd_128_emb_300_rdrp_0.2_drp_0.2_sch_reducelronplateau_cycl_no...</td>\n",
       "      <td>0.812697</td>\n",
       "      <td>0.665538</td>\n",
       "      <td>0.636402</td>\n",
       "      <td>0.627120</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_128_emb_100_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_...</td>\n",
       "      <td>0.830050</td>\n",
       "      <td>0.659452</td>\n",
       "      <td>0.625518</td>\n",
       "      <td>0.618985</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_128_emb_300_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_...</td>\n",
       "      <td>0.844914</td>\n",
       "      <td>0.653402</td>\n",
       "      <td>0.623495</td>\n",
       "      <td>0.615015</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_pr...</td>\n",
       "      <td>0.842095</td>\n",
       "      <td>0.653221</td>\n",
       "      <td>0.621605</td>\n",
       "      <td>0.614682</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_64_emb_300_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_p...</td>\n",
       "      <td>0.855385</td>\n",
       "      <td>0.644542</td>\n",
       "      <td>0.621185</td>\n",
       "      <td>0.615025</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_64_emb_100_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_p...</td>\n",
       "      <td>0.848828</td>\n",
       "      <td>0.652501</td>\n",
       "      <td>0.620857</td>\n",
       "      <td>0.614475</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_128_nl_2_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_yes_p...</td>\n",
       "      <td>0.845659</td>\n",
       "      <td>0.652105</td>\n",
       "      <td>0.616664</td>\n",
       "      <td>0.611597</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_128_nl_2_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_pr...</td>\n",
       "      <td>0.852391</td>\n",
       "      <td>0.649080</td>\n",
       "      <td>0.612978</td>\n",
       "      <td>0.606741</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_yes_p...</td>\n",
       "      <td>0.842435</td>\n",
       "      <td>0.653149</td>\n",
       "      <td>0.612381</td>\n",
       "      <td>0.607020</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_128_nl_1_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_pr...</td>\n",
       "      <td>0.864591</td>\n",
       "      <td>0.641841</td>\n",
       "      <td>0.607943</td>\n",
       "      <td>0.602349</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rnn_lr_0.001_wdc_0.01_bsz_128_nl_1_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_yes_p...</td>\n",
       "      <td>0.852371</td>\n",
       "      <td>0.648216</td>\n",
       "      <td>0.606355</td>\n",
       "      <td>0.599965</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              modelname  \\\n",
       "0      han_lr_0.0005_wdc_0.05_bsz_512_whd_128_shd_128_emb_300_drp_0.2_sch_cycliclr_cycl_4_lrp_no_pre_no   \n",
       "1   han_lr_0.001_wdc_0.05_bsz_512_whd_128_shd_128_emb_300_drp_0.2_sch_reducelronplateau_cycl_no_lrp_...   \n",
       "2   han_lr_0.001_wdc_0.01_bsz_64_whd_64_shd_64_emb_300_drp_0.2_sch_reducelronplateau_cycl_no_lrp_2_p...   \n",
       "3              han_lr_0.001_wdc_0.01_bsz_128_whd_64_shd_64_emb_300_drp_0.2_sch_no_cycl_no_lrp_no_pre_no   \n",
       "4      han_lr_0.0005_wdc_0.05_bsz_512_whd_128_shd_128_emb_300_drp_0.2_sch_cycliclr_cycl_2_lrp_no_pre_no   \n",
       "5               han_lr_0.001_wdc_0.01_bsz_64_whd_64_shd_64_emb_300_drp_0.2_sch_no_cycl_no_lrp_no_pre_no   \n",
       "6              han_lr_0.001_wdc_0.01_bsz_512_whd_64_shd_64_emb_300_drp_0.2_sch_no_cycl_no_lrp_no_pre_no   \n",
       "7               han_lr_0.001_wdc_0.01_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "8               han_lr_0.001_wdc_0.01_bsz_128_whd_64_shd_64_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "9              han_lr_0.001_wdc_0.05_bsz_128_whd_32_shd_32_emb_100_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "10        han_lr_0.0005_wdc_0.01_bsz_64_whd_64_shd_64_emb_300_drp_0.2_sch_cycliclr_cycl_1_lrp_no_pre_no   \n",
       "11             han_lr_0.001_wdc_0.05_bsz_128_whd_32_shd_32_emb_300_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "12           han_lr_0.001_wdc_0.01_bsz_512_whd_128_shd_128_emb_300_drp_0.5_sch_no_cycl_no_lrp_no_pre_no   \n",
       "13        han_lr_0.0005_wdc_0.01_bsz_64_whd_64_shd_64_emb_300_drp_0.2_sch_cycliclr_cycl_4_lrp_no_pre_no   \n",
       "14     han_lr_0.0005_wdc_0.05_bsz_512_whd_128_shd_128_emb_300_drp_0.2_sch_cycliclr_cycl_1_lrp_no_pre_no   \n",
       "15            han_lr_0.001_wdc_0.01_bsz_128_whd_128_shd_128_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "16             han_lr_0.001_wdc_0.05_bsz_128_whd_32_shd_32_emb_200_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "17               han_lr_0.001_wdc_0.01_bsz_32_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "18           han_lr_0.001_wdc_0.01_bsz_128_whd_128_shd_128_emb_300_drp_0.5_sch_no_cycl_no_lrp_no_pre_no   \n",
       "19               han_lr_0.001_wdc_0.01_bsz_64_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "20              han_lr_0.001_wdc_0.01_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "21              han_lr_0.001_wdc_0.05_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "22        han_lr_0.0005_wdc_0.01_bsz_64_whd_64_shd_64_emb_300_drp_0.2_sch_cycliclr_cycl_2_lrp_no_pre_no   \n",
       "23            han_lr_0.001_wdc_0.01_bsz_64_whd_128_shd_128_emb_300_drp_0.5_sch_no_cycl_no_lrp_no_pre_no   \n",
       "24              han_lr_0.001_wdc_0.01_bsz_128_whd_16_shd_16_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "25              han_lr_0.001_wdc_0.01_bsz_256_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "26              han_lr_0.001_wdc_0.01_bsz_512_whd_32_shd_32_emb_50_drp_0.0_sch_no_cycl_no_lrp_no_pre_no   \n",
       "27  rnn_lr_0.0005_wdc_0.05_bsz_512_nl_3_hd_128_emb_300_rdrp_0.2_drp_0.2_sch_cycliclr_cycl_4_lrp_no_a...   \n",
       "28  rnn_lr_0.001_wdc_0.01_bsz_512_nl_3_hd_128_emb_300_rdrp_0.2_drp_0.2_sch_no_cycl_no_lrp_no_att_no_...   \n",
       "29  rnn_lr_0.001_wdc_0.05_bsz_512_nl_3_hd_128_emb_300_rdrp_0.2_drp_0.2_sch_reducelronplateau_cycl_no...   \n",
       "30  rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_128_emb_100_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_...   \n",
       "31  rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_128_emb_300_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_...   \n",
       "32  rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_pr...   \n",
       "33  rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_64_emb_300_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_p...   \n",
       "34  rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_64_emb_100_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_p...   \n",
       "35  rnn_lr_0.001_wdc_0.01_bsz_128_nl_2_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_yes_p...   \n",
       "36  rnn_lr_0.001_wdc_0.01_bsz_128_nl_2_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_pr...   \n",
       "37  rnn_lr_0.001_wdc_0.01_bsz_128_nl_3_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_yes_p...   \n",
       "38  rnn_lr_0.001_wdc_0.01_bsz_128_nl_1_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_no_pr...   \n",
       "39  rnn_lr_0.001_wdc_0.01_bsz_128_nl_1_hd_32_emb_50_rdrp_0.0_drp_0.0_sch_no_cycl_no_lrp_no_att_yes_p...   \n",
       "\n",
       "        loss       acc        f1      prec  best_epoch  \n",
       "0   0.676955  0.722332  0.709809  0.703325           3  \n",
       "1   0.672373  0.720424  0.707064  0.700494           6  \n",
       "2   0.677667  0.720460  0.706726  0.701034           4  \n",
       "3   0.679628  0.721108  0.706106  0.700320           6  \n",
       "4   0.684018  0.718407  0.704798  0.698459           5  \n",
       "5   0.681003  0.716930  0.703556  0.698334           4  \n",
       "6   0.684646  0.718155  0.702430  0.696197           7  \n",
       "7   0.702976  0.708827  0.698351  0.693251           3  \n",
       "8   0.693445  0.711528  0.698188  0.694117           2  \n",
       "9   0.696123  0.712897  0.698097  0.692957           2  \n",
       "10  0.695813  0.712392  0.697468  0.690748           3  \n",
       "11  0.695857  0.712176  0.697281  0.690789           1  \n",
       "12  0.690965  0.713725  0.697224  0.689717          15  \n",
       "13  0.689646  0.715201  0.697202  0.690485           3  \n",
       "14  0.686094  0.716894  0.697059  0.691581           4  \n",
       "15  0.699591  0.710880  0.696889  0.693631           2  \n",
       "16  0.697555  0.709691  0.696652  0.690477           1  \n",
       "17  0.692085  0.713797  0.695815  0.690835           2  \n",
       "18  0.691776  0.710160  0.695556  0.688225          11  \n",
       "19  0.696436  0.708575  0.694300  0.688867           2  \n",
       "20  0.702082  0.707531  0.694211  0.689066           3  \n",
       "21  0.696131  0.707639  0.694087  0.688432           3  \n",
       "22  0.702785  0.705658  0.692895  0.686547           1  \n",
       "23  0.696708  0.711132  0.691840  0.684800           8  \n",
       "24  0.711794  0.706774  0.690902  0.683959           3  \n",
       "25  0.703306  0.711348  0.689721  0.683954           4  \n",
       "26  0.702623  0.709115  0.689221  0.682835           5  \n",
       "27  0.799264  0.671120  0.642041  0.635197          19  \n",
       "28  0.810204  0.665250  0.636496  0.627285          18  \n",
       "29  0.812697  0.665538  0.636402  0.627120          17  \n",
       "30  0.830050  0.659452  0.625518  0.618985           4  \n",
       "31  0.844914  0.653402  0.623495  0.615015           3  \n",
       "32  0.842095  0.653221  0.621605  0.614682           9  \n",
       "33  0.855385  0.644542  0.621185  0.615025           4  \n",
       "34  0.848828  0.652501  0.620857  0.614475           5  \n",
       "35  0.845659  0.652105  0.616664  0.611597           9  \n",
       "36  0.852391  0.649080  0.612978  0.606741          11  \n",
       "37  0.842435  0.653149  0.612381  0.607020           8  \n",
       "38  0.864591  0.641841  0.607943  0.602349          14  \n",
       "39  0.852371  0.648216  0.606355  0.599965          19  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pytorch = pd.read_csv(results_path / pytorch_fname)\n",
    "res_pytorch.sort_values(['f1'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will noticed the lengthy names. Again, this is not the best way to \"store\" the set-up used per experiment. In the `main_pytorch.py` script I have included a piece of code that is a more elegant solution if one wanted to automate the process. For the relatively quick experimentation I wanted to do here these lengthy names are more convenient. \n",
    "\n",
    "Let me explain how one has to read them, for example: \n",
    "\n",
    "`han_lr_0.0005_wdc_0.05_bsz_512_whd_128_shd_128_emb_300_drp_0.2_sch_cycliclr_cycl_4_lrp_no_pre_no`\n",
    "\n",
    "This run corresponds to a HAN model (RNN is also available) with:\n",
    "\n",
    "* learning rate (lr) = 0.0005\n",
    "* weight decay (wdc) = 0.05\n",
    "* batch size (bsz) = 0.05 \n",
    "* Word GRU hidden dim (whd) = 128\n",
    "* sentence GRU hidden dim (shd) = 128\n",
    "* word embedding dim (emb) = 300\n",
    "* Embedding, Weight, Locked and \"Last\" dropout (drp) = 0.2\n",
    "* using a cyclic learning rate scheduler (sch) with 4 cycles (cycl) with lr ranging from 0.0005 to 0.005\n",
    "* Learning rate patience (lrp) does not apply since is only used when using ReduceLrOnPlateau\n",
    "* No pretrained (pre) word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelname</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>best_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.0_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.713377</td>\n",
       "      <td>0.702921</td>\n",
       "      <td>0.694185</td>\n",
       "      <td>0.693122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.0_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_multifactorscheduler_step_248_pr...</td>\n",
       "      <td>0.713377</td>\n",
       "      <td>0.702921</td>\n",
       "      <td>0.694185</td>\n",
       "      <td>0.693122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.0_bsz_256_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.707275</td>\n",
       "      <td>0.703785</td>\n",
       "      <td>0.685044</td>\n",
       "      <td>0.684165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.0_bsz_128_whd_32_shd_32_emb_50_drp_0.2_sch_no_step_no_pre_no</td>\n",
       "      <td>0.717142</td>\n",
       "      <td>0.695394</td>\n",
       "      <td>0.681730</td>\n",
       "      <td>0.681444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.0_bsz_128_whd_64_shd_64_emb_300_drp_0.2_sch_no_step_no_pre_no</td>\n",
       "      <td>0.753628</td>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.669988</td>\n",
       "      <td>0.662521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.0_bsz_64_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.731422</td>\n",
       "      <td>0.699247</td>\n",
       "      <td>0.669075</td>\n",
       "      <td>0.663576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.0_bsz_512_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.709781</td>\n",
       "      <td>0.704685</td>\n",
       "      <td>0.668295</td>\n",
       "      <td>0.670837</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_128_whd_64_shd_64_emb_200_drp_0.0_sch_multifactorscheduler_step_248...</td>\n",
       "      <td>0.729639</td>\n",
       "      <td>0.696834</td>\n",
       "      <td>0.666595</td>\n",
       "      <td>0.660634</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_512_whd_32_shd_32_emb_50_drp_0.0_sch_multifactorscheduler_step_248_...</td>\n",
       "      <td>0.729468</td>\n",
       "      <td>0.693737</td>\n",
       "      <td>0.665710</td>\n",
       "      <td>0.658832</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_128_whd_16_shd_16_emb_50_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.753661</td>\n",
       "      <td>0.685922</td>\n",
       "      <td>0.664380</td>\n",
       "      <td>0.656559</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_128_whd_32_shd_32_emb_200_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.752646</td>\n",
       "      <td>0.686678</td>\n",
       "      <td>0.662508</td>\n",
       "      <td>0.654337</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_128_whd_32_shd_32_emb_100_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.752289</td>\n",
       "      <td>0.687759</td>\n",
       "      <td>0.661425</td>\n",
       "      <td>0.654459</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_128_whd_64_shd_64_emb_50_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.753855</td>\n",
       "      <td>0.685454</td>\n",
       "      <td>0.659264</td>\n",
       "      <td>0.651740</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.752302</td>\n",
       "      <td>0.685814</td>\n",
       "      <td>0.658699</td>\n",
       "      <td>0.651851</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.752302</td>\n",
       "      <td>0.685814</td>\n",
       "      <td>0.658699</td>\n",
       "      <td>0.651851</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_128_whd_32_shd_32_emb_300_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.763201</td>\n",
       "      <td>0.680916</td>\n",
       "      <td>0.647853</td>\n",
       "      <td>0.642377</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.0_bsz_32_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.770682</td>\n",
       "      <td>0.676667</td>\n",
       "      <td>0.642401</td>\n",
       "      <td>0.633570</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_128_whd_128_shd_128_emb_50_drp_0.0_sch_no_step_no_pre_no</td>\n",
       "      <td>0.827354</td>\n",
       "      <td>0.647135</td>\n",
       "      <td>0.599105</td>\n",
       "      <td>0.582242</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>han_mx_lr_0.01_wdc_0.001_bsz_128_whd_128_shd_128_emb_300_drp_0.2_sch_no_step_no_pre_no</td>\n",
       "      <td>0.858164</td>\n",
       "      <td>0.631829</td>\n",
       "      <td>0.568654</td>\n",
       "      <td>0.560839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              modelname  \\\n",
       "0                     han_mx_lr_0.01_wdc_0.0_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no   \n",
       "1   han_mx_lr_0.01_wdc_0.0_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_multifactorscheduler_step_248_pr...   \n",
       "2                     han_mx_lr_0.01_wdc_0.0_bsz_256_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no   \n",
       "3                     han_mx_lr_0.01_wdc_0.0_bsz_128_whd_32_shd_32_emb_50_drp_0.2_sch_no_step_no_pre_no   \n",
       "4                    han_mx_lr_0.01_wdc_0.0_bsz_128_whd_64_shd_64_emb_300_drp_0.2_sch_no_step_no_pre_no   \n",
       "5                      han_mx_lr_0.01_wdc_0.0_bsz_64_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no   \n",
       "6                     han_mx_lr_0.01_wdc_0.0_bsz_512_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no   \n",
       "7   han_mx_lr_0.01_wdc_0.001_bsz_128_whd_64_shd_64_emb_200_drp_0.0_sch_multifactorscheduler_step_248...   \n",
       "8   han_mx_lr_0.01_wdc_0.001_bsz_512_whd_32_shd_32_emb_50_drp_0.0_sch_multifactorscheduler_step_248_...   \n",
       "9                   han_mx_lr_0.01_wdc_0.001_bsz_128_whd_16_shd_16_emb_50_drp_0.0_sch_no_step_no_pre_no   \n",
       "10                 han_mx_lr_0.01_wdc_0.001_bsz_128_whd_32_shd_32_emb_200_drp_0.0_sch_no_step_no_pre_no   \n",
       "11                 han_mx_lr_0.01_wdc_0.001_bsz_128_whd_32_shd_32_emb_100_drp_0.0_sch_no_step_no_pre_no   \n",
       "12                  han_mx_lr_0.01_wdc_0.001_bsz_128_whd_64_shd_64_emb_50_drp_0.0_sch_no_step_no_pre_no   \n",
       "13                  han_mx_lr_0.01_wdc_0.001_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no   \n",
       "14                  han_mx_lr_0.01_wdc_0.001_bsz_128_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no   \n",
       "15                 han_mx_lr_0.01_wdc_0.001_bsz_128_whd_32_shd_32_emb_300_drp_0.0_sch_no_step_no_pre_no   \n",
       "16                     han_mx_lr_0.01_wdc_0.0_bsz_32_whd_32_shd_32_emb_50_drp_0.0_sch_no_step_no_pre_no   \n",
       "17                han_mx_lr_0.01_wdc_0.001_bsz_128_whd_128_shd_128_emb_50_drp_0.0_sch_no_step_no_pre_no   \n",
       "18               han_mx_lr_0.01_wdc_0.001_bsz_128_whd_128_shd_128_emb_300_drp_0.2_sch_no_step_no_pre_no   \n",
       "\n",
       "        loss       acc        f1      prec  best_epoch  \n",
       "0   0.713377  0.702921  0.694185  0.693122           0  \n",
       "1   0.713377  0.702921  0.694185  0.693122           0  \n",
       "2   0.707275  0.703785  0.685044  0.684165           0  \n",
       "3   0.717142  0.695394  0.681730  0.681444           0  \n",
       "4   0.753628  0.682861  0.669988  0.662521           0  \n",
       "5   0.731422  0.699247  0.669075  0.663576           0  \n",
       "6   0.709781  0.704685  0.668295  0.670837           1  \n",
       "7   0.729639  0.696834  0.666595  0.660634          17  \n",
       "8   0.729468  0.693737  0.665710  0.658832           8  \n",
       "9   0.753661  0.685922  0.664380  0.656559          11  \n",
       "10  0.752646  0.686678  0.662508  0.654337          11  \n",
       "11  0.752289  0.687759  0.661425  0.654459           7  \n",
       "12  0.753855  0.685454  0.659264  0.651740          10  \n",
       "13  0.752302  0.685814  0.658699  0.651851           7  \n",
       "14  0.752302  0.685814  0.658699  0.651851           7  \n",
       "15  0.763201  0.680916  0.647853  0.642377           5  \n",
       "16  0.770682  0.676667  0.642401  0.633570           0  \n",
       "17  0.827354  0.647135  0.599105  0.582242           2  \n",
       "18  0.858164  0.631829  0.568654  0.560839           1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_mxnet = pd.read_csv(results_path / mxnet_fname)\n",
    "res_mxnet.sort_values(['f1'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "#### Overfitting and Dropout\n",
    "\n",
    "As I mentioned in the previous notebook, when I started to run experiments I noticed that the model overfitted quite early, moreover in the case of `Mxnet` (I will come back to the `MxNet` implemenation later). To mitigate that I used a number of dropout mechanisms (see Notebook 03) along with exploring relatively simple architectures and early stopping. Still, the best loss/metrics are reached quite early during the fitting process. However, these are not too distant of the best loss/training metrics. In other words, the overfitting is not that important after a small number of epochs. Of course, there is still more to try, like even simpler architectures, starting with lower learning rates or higher weight decay values. I will leave this for future exploration (or to you, the reader, if you are willing to 🙂)\n",
    "\n",
    "The fact that the model quickly finds the best solution is not neccesarily bad. It might mean that the problem is relatively simple for the algorithm. For example, maybe only a small number of tokens are required to make good decisions (we'll see this in the next and final notebook). Also, maybe a better preprocessing would also help to reduce overfitting (I will also discuss this a bit more in the next notebook)\n",
    "\n",
    "#### Hierarchy and Attention \n",
    "\n",
    "In the code in this repo I also include the option of using a \"simple\" stack of RNNs (LSTMs, referred as RNN model) with or without Attention, ignoring word/sentence hierarchy. The truth is that, when using the RNN model, I have explored the parameter space less than with the HAN model. However, the results seem to suggest that Hierarchy does make a difference. Test losses and metrics are worse when using the RNN model compared to those obtained using the HAN model. However, one might also observe in the table that the RNN model overfits less than the HAN model, in some cases, metrics are still improving after 20 epochs (the maximum number of epochs per experiment). This along with the already mentioned fact that I have not properly explored the parameter space in the case of the RNN model suggests that better results are still attainable. \n",
    "\n",
    "For example, when using the RNN model I have only used Attention without context. Maybe using a context vector makes a significant difference 🤷🏻‍♂️. As with some other comments I made throughout the notebooks, I will try a few more set ups when I re-visit this repo. \n",
    "\n",
    "#### MxNet\n",
    "\n",
    "The code in this repo represents my second dive into `MxNet` (after the implementation of neural collaborative filtering [here](https://github.com/jrzaurin/neural_cf)). The more I use this DL frame the more I like it. Is written in a way that \"makes sense\" and, even I still don't think is as mature as `Pytorch`, I can see it catching up soon. Also, the last sentence might be of course influenced by the fact that I have not used `MxNet` as much as I have used `Pytorch`. With that in mind I will start by saying that the exploration process has been a bit lighter with `MxNet`. The results that I obtained where a bit worse than those with `Pytorch`, but I am sure that has to do with the fact that I do not know some of the details behing `MxNet`'s implementations. Therefore, if you, reading these lines, are an `MxNet` expert or simply know about `MxNet`, I would love to have a chat with you about my `MxNet` implementation of HANs. \n",
    "\n",
    "#### Results\n",
    "\n",
    "So far, the best results I obtained when predicting the reviews score where using *tf-idf* + `LightGBM` properly tuned using `Hyperopt`. The metrics where: \n",
    "\n",
    "| acc | f1 | pre  |\n",
    "|---|---|---|---|---|\n",
    "| 0.7054  | 0.6832  |  0.676335  |\n",
    "\n",
    "Here, the best results are:\n",
    "\n",
    "| acc | f1 | pre  |\n",
    "|---|---|---|---|---|\n",
    "| 0.7223  | 0.71  |  0.7033  |\n",
    "\n",
    "These results are obtained in less than 10 epochs with a model that trains in less than 2 min per epoch on a Tesla K80 GPU. \n",
    "\n",
    "Of course, the question is, it is worth chosing HAN over a tf-idf + LightGBM? To me the answer is, as with most things in life, \"it depends\". The increase is not that big, but it is significant. For example, if in your business getting a $\\sim$3 increase in F1 score represent a sizeable increase in revenue or savings, then there is no question. Also, as we will see in the next notebook the attention weights might provide you with insights into which expressions or semantic constructions are relevant within the text, beyond keywords. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
