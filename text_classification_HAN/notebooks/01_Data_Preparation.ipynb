{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸš¨**NOTE** ðŸš¨: to run the notebooks move them to the main dir. Simply\n",
    "\n",
    "```bash\n",
    "cp notebook_name.ipynd ../\n",
    "```\n",
    "\n",
    "In this and the other notebooks I will describe step by step the my implemenation of [Hierarchical Attention Networks](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf) (Zichao Yang et al., 2016) and discuss the results I obtained for the [amazon reviews dataset](https://cseweb.ucsd.edu/~jmcauley/pdfs/www16a.pdf) (R. He, J. McAuley, 2016), in particular [Clothes, shoes and jewellery](http://jmcauley.ucsd.edu/data/amazon/).\n",
    "\n",
    "An illustration of the architecture I will implement is shown in the figure below (the Figure 2 in the Zichao Yang et al., 2016 paper)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"400\" src=\"figures/HAN_arch.png\">\n",
    "</p>\n",
    "\n",
    "In words, we consider a document comprised by a number of sentences $L$, and each sentence contains a number of tokens $T$. The network first encodes the tokens ($w_{it}, i \\in [1,T]$) in each sentence $s_i, i \\in [1,L]$ using an RNN, in particular a GRU (Word Encoder), with an attention mechanism (Word Attention). The result of this first step is a sentence representation. This sentence representation is then passed through a second GRU (Sentence Encoder) with attention (Sentence Attention). The result of this second step is a document (in this exercises documents are reviews) representation. The latter is \"fed to\" a fully connected layer for prediction (Softmax in the figure). \n",
    "\n",
    "Let me give you one example with tensor dimensions. Let's assume tha we have a document comprised by 10 sentences each of 20 words, and that we use 100 dim word embeddings. The output of Word-Encoder + Word-Attention will be a tensor of shape (None, 10, 100), while the output of the Sentence-Encoder + Sentence-Attention will be a tensor of shape (None, 1, 100). The latter will be the input of a fully connected (i.e. prediction) layer. \n",
    "\n",
    "Taken directly from their paper, the **word attention mechanism** can be formulated as:\n",
    "\n",
    "$$\n",
    "u_{it} = \\text{tanh}(W_wh_{it} + b_w)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{it} = \\frac{\\exp(u_{it}u_w^{\\mathsf{T}})}{\\sum_{t}\\exp(u_{it}u_w^{\\mathsf{T}})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "s_i = \\sum_{t}\\alpha_{it}h_{it}\n",
    "$$\n",
    "\n",
    "Where $u_{it}$ can be seen as a hidden representation of $h_{it}$ (the GRU ouput). The importance of a word is then measured as the similarity of $u_{it}$ with a context vector $u_{w}$, which is then normalized through a softmax function resulting in  $\\alpha_{it}$, the so called *normalized importance weight*. The sentence vector $s_i$ is the weighted sum of the word annotations based on the weights $\\alpha_{it}$. For more details please, have a look to the paper [Zichao Yang et al., 2016](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf). \n",
    "\n",
    "The **sentence attention mechanism** the authors used is identical to the previous one, but at sentence level representation:\n",
    "\n",
    "$$\n",
    "u_{i} = \\text{tanh}(W_sh_i + b_s)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{i} = \\frac{\\exp(u_iu_s^{\\mathsf{T}})}{\\sum_{i}\\exp(u_{i}u_s^{\\mathsf{T}})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v = \\sum_{i}\\alpha_{i}h_{i}\n",
    "$$\n",
    "\n",
    "I will come back to these expressions in the next notebook, where I will implement them in code. \n",
    "\n",
    "However, and as with any other data problem, the first step is preparing the data. For this particular exercises, we need to tokenize reviews into sentences and sentences into tokens. Of course, this is easily attainable using [`Spacy`](https://spacy.io/usage) or any other of your NLP favourite packages. \n",
    "\n",
    "To make life easier I have wrapped up the full preprocessing in a class called `HANPreprocessor` in the `utils` module. One could access to this class by simply:\n",
    "\n",
    "```python\n",
    "from utils.preprocessors import HANPreprocessor\n",
    "```\n",
    "\n",
    "Here I will describe step by step what happens inside this class. Let's start by reading the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.utils import tokenize\n",
    "from fastai.text import Tokenizer, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../datasets/amazon_reviews/\")\n",
    "OUT_PATH = Path(\"data/\")\n",
    "if not os.path.exists(OUT_PATH):\n",
    "    os.makedirs(OUT_PATH)\n",
    "\n",
    "df_org = pd.read_json(DATA_PATH / \"reviews_Clothing_Shoes_and_Jewelry_5.json.gz\", lines=True)\n",
    "\n",
    "# classes from [0,num_class)\n",
    "df = df_org.copy()\n",
    "df[\"overall\"] = (df[\"overall\"] - 1).astype(\"int64\")\n",
    "\n",
    "# group reviews with 1 and 2 scores into one class\n",
    "df.loc[df.overall == 0, \"overall\"] = 1\n",
    "\n",
    "# and back again to [0,num_class)\n",
    "df[\"overall\"] = (df[\"overall\"] - 1).astype(\"int64\")\n",
    "\n",
    "# agressive preprocessing: drop short reviews\n",
    "df[\"reviewLength\"] = df.reviewText.apply(lambda x: len(x.split(\" \")))\n",
    "df = df[df.reviewLength >= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of the notebooks is to illustrate the details of the process (rather than running real experiments). Therefore, with that in mind, I will only use 1000 samples here. Otherwise tokenizing reviews into sentences and sentences into tokens takes a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.sample(n=1000, random_state=1)\n",
    "texts = sdf.reviewText.tolist()\n",
    "target = sdf.overall.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the `HANPreprocessor` class, there is a private method called `_sentencizer` that does mostly this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_func = spacy.load(\"en_core_web_sm\")\n",
    "n_cpus = os.cpu_count()\n",
    "bsz = 100\n",
    "texts_sents = []\n",
    "for doc in tok_func.pipe(texts, n_process=n_cpus, batch_size=bsz):\n",
    "    sents = [str(s) for s in list(doc.sents)]\n",
    "    texts_sents.append(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is and original design for the beach.',\n",
       " 'Super light and it also covers very well.',\n",
       " 'I got XL.',\n",
       " \"I'm  5'11&#34; and size 16 and it works.\",\n",
       " 'Length is a little above knee.',\n",
       " 'there are others, but sizes are not true and this one has better fabric quality.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the sentences, we would like to process the text in them. However, for the whole dataset there are over 1.4 mil sentences and looping through reviews and then sentences is just **BAD**. Let's implement a better solution. First we flat out the nested lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nested to flat list.\n",
    "all_sents = [s for sents in texts_sents for s in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the lengths of the original documents (number of sentences) and build a list with the sentence indexes that belong to each document, so we can \"fold back\" the list later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the lengths of the documents: 1) for padding purposes and 2) to compute consecutive ranges \n",
    "# so we can \"fold\" the list again\n",
    "texts_length = [0] + [len(s) for s in texts_sents]\n",
    "range_idx = [sum(texts_length[: i + 1]) for i in range(len(texts_length))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process the text in `all_sents` in parallel using the fantastic [`fastai`](https://github.com/fastai/fastai/blob/96339e70184eeed8d28261a88be54631eafc77cf/fastai/text/transform.py#L87) tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_tokens = Tokenizer().process_all(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxmaj',\n",
       " 'this',\n",
       " 'is',\n",
       " 'and',\n",
       " 'original',\n",
       " 'design',\n",
       " 'for',\n",
       " 'the',\n",
       " 'beach',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I apply very little cleaning to the text (other than the one applied by default by the fastai Tokenizer). This in **intentional**, as I aim to keep the maximum information possible within the text and hopefully the \"noise\" will be removed when removing the least frequent tokens from the vocabulary. \n",
    "\n",
    "Nonetheless, I have included the option of applying some cleaning via a mildly customised `gensim`'s [`simple_preprocess`](https://radimrehurek.com/gensim/utils.html). All this is wrapped up in a function within the `utils.preprocessors` called `get_texts` that is mostly this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocess(doc, lower=False, deacc=False, min_len=2, max_len=15):\n",
    "    tokens = [\n",
    "        token\n",
    "        for token in tokenize(doc, lower=False, deacc=deacc, errors=\"ignore\")\n",
    "        if min_len <= len(token) <= max_len and not token.startswith(\"_\")\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_texts(texts, with_preprocess=False):\n",
    "    if with_preprocess:\n",
    "        texts = [\" \".join(simple_preprocess(s)) for s in texts]\n",
    "    tokens = Tokenizer().process_all(texts)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_tokens_2 = get_texts(all_sents, with_preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxmaj', 'this', 'is', 'and', 'original', 'design', 'for', 'the', 'beach']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_tokens_2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the punctuation has dissapeared, which is good. However, let's have a look to this sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"I don't: particularly ; like data science.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'do', \"n't\", ':', 'particularly', ';', 'like', 'data', 'science', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_texts([s], with_preprocess=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don', 'particularly', 'like', 'data', 'science']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_texts([s], with_preprocess=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see that the negation has dissapeared, which is not something I want here. If one wanted to remove some punctuation while keeping negations, the solution is rather simple, moreover, given the structure of the `fastai`'s `Tokenizer`. You could define a function like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rm_punctuation(x):\n",
    "    x = re.sub(r\"\\.|,|:|;\", \" \", x)\n",
    "    # or \n",
    "    # x = x.replace(\".\", \"\").replace(\",\",\"\").replace(\":\", \"\").replace(\";\",\"\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tok = Tokenizer()\n",
    "new_tok.pre_rules = [rm_punctuation] + new_tok.pre_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this way you could take full advantage of all the `fastai`'s preprocessing rules and simply add a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'do', \"n't\", 'particularly', 'like', 'data', 'science']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tok.process_all([s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will leave that for future implementations if/when I revisit this exercise. The full vocabulary for these reviews, applying a low freq cut of 5, is around 22k words (i.e. not too big). Therefore, worst case scenario, the network will end up learning a few useless embeddings (as we will see).  \n",
    "\n",
    "Let's move on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â saving the lengths of sentences for padding purposes\n",
    "sents_length = [len(s) for s in sents_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary using fastai's Vocab class\n",
    "vocab = Vocab.create(sents_tokens, max_vocab=5000, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'numericalize' each sentence\n",
    "sents_numz = [vocab.numericalize(s) for s in sents_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the sentences again into documents\n",
    "texts_numz = [sents_numz[range_idx[i] : range_idx[i + 1]] for i in range(len(range_idx[:-1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 19, 17, 13, 703, 320, 18, 10, 950, 9],\n",
       " [5, 268, 216, 13, 15, 105, 1072, 39, 64, 9],\n",
       " [11, 112, 6, 491, 9],\n",
       " [11, 79, 0, 51, 13, 44, 951, 13, 15, 453, 9],\n",
       " [5, 227, 17, 14, 80, 492, 952, 9],\n",
       " [115, 24, 493, 12, 25, 321, 24, 31, 236, 13, 19, 59, 108, 160, 237, 88, 9]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_numz[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if all is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxmaj', 'super', 'light', 'and', 'it', 'also', 'covers', 'very', 'well', '.']\n",
      "['xxmaj', 'super', 'light', 'and', 'it', 'also', 'covers', 'very', 'well', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sents_tokens[1])\n",
    "print([vocab.itos[i] for i in texts_numz[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all sentences tokenized and processed, the only thing left is to pad them so they are ready to be passed to the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=0.8\n",
    "maxlen_sent = int(np.quantile(sents_length, q=q))\n",
    "maxlen_doc  = int(np.quantile(texts_length[1:], q=q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 7\n"
     ]
    }
   ],
   "source": [
    "print(maxlen_sent, maxlen_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sentences will be pad to 21 tokens and all documents/reviews to 7 sentences. \n",
    "\n",
    "I have coded two helper functions to help with the padding. These are inspired by code in the fastai library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(seq, maxlen, pad_first=True, pad_idx=1):\n",
    "    if len(seq) >= maxlen:\n",
    "        res = np.array(seq[-maxlen:]).astype(\"int32\")\n",
    "        return res\n",
    "    else:\n",
    "        res = np.zeros(maxlen, dtype=\"int32\") + pad_idx\n",
    "        if pad_first:\n",
    "            res[-len(seq) :] = seq\n",
    "        else:\n",
    "            res[: len(seq) :] = seq\n",
    "        return res\n",
    "\n",
    "\n",
    "def pad_nested_sequences(\n",
    "    seq, maxlen_sent, maxlen_doc, pad_sent_first=True, pad_doc_first=False, pad_idx=1\n",
    "):\n",
    "    seq = [s for s in seq if len(s) >= 1]\n",
    "    if len(seq) == 0:\n",
    "        return np.array([[pad_idx] * maxlen_sent] * maxlen_doc).astype(\"int32\")\n",
    "    seq = [pad_sequences(s, maxlen_sent, pad_sent_first, pad_idx) for s in seq]\n",
    "    if len(seq) >= maxlen_doc:\n",
    "        return np.array(seq[:maxlen_doc])\n",
    "    else:\n",
    "        res = np.array([[pad_idx] * maxlen_sent] * maxlen_doc).astype(\"int32\")\n",
    "        if pad_doc_first:\n",
    "            res[-len(seq) :] = seq\n",
    "        else:\n",
    "            res[: len(seq) :] = seq\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_texts = np.stack([pad_nested_sequences(r, maxlen_sent, maxlen_doc) for r in texts_numz], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7, 21)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[   1,    1,    1,    1, ...,   18,   10,  950,    9],\n",
       "        [   1,    1,    1,    1, ..., 1072,   39,   64,    9],\n",
       "        [   1,    1,    1,    1, ...,  112,    6,  491,    9],\n",
       "        [   1,    1,    1,    1, ...,   13,   15,  453,    9],\n",
       "        [   1,    1,    1,    1, ...,   80,  492,  952,    9],\n",
       "        [   1,    1,    1,    1, ...,  160,  237,   88,    9],\n",
       "        [   1,    1,    1,    1, ...,    1,    1,    1,    1]],\n",
       "\n",
       "       [[  11,  142,  249,   10, ...,   72,   42,   15,    9],\n",
       "        [   1,    1,    1,    1, ...,   20,   19,  139,    9],\n",
       "        [   1,    1,    5,  300, ...,   23,   10,  467,    9],\n",
       "        [   1,    1,    1,    5, ...,  315,    7,  135,    9],\n",
       "        [   1,    1,    1,    1, ...,    0,   21,  315,    9],\n",
       "        [   0,  183,   10,  274, ...,  643,   16,    0,    9],\n",
       "        [ 156,   18,  495,   49, ...,  494,  282,  274,    9]],\n",
       "\n",
       "       [[   1,    1,    1,    1, ...,    5,    0,  797,    9],\n",
       "        [   1,    1,    1,    1, ...,  222,  169,   57,    9],\n",
       "        [   1,    1,    1,    1, ...,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    1, ...,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    1, ...,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    1, ...,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    1, ...,    1,    1,    1,    1]],\n",
       "\n",
       "       [[   1,    1,    1,    1, ...,   98,   20,   28,    9],\n",
       "        [   1,    5,   22,   24, ...,   38,  954,  454,    9],\n",
       "        [ 955,   51,   32,   95, ...,   34,  196,  956,    9],\n",
       "        [   1,    1,    1,    1, ...,   16,   75,  348,  468],\n",
       "        [  25,   11,  142,  172, ...,  431,  170,  605,    9],\n",
       "        [   0,   83,  749,    0, ...,  172,   14,    0,    9],\n",
       "        [   1,    1,    1,    1, ...,    1,    1,    1,    1]],\n",
       "\n",
       "       [[   1,    1,    1,    1, ...,    0,  274,  382,    9],\n",
       "        [   1,    1,    1,    1, ...,  157,   57,   84,    9],\n",
       "        [   1,    1,    1,    1, ...,   16,   43,    0,    9],\n",
       "        [  25,   97,   17,   10, ...,   20,  117,   34,  232],\n",
       "        [  10,    5,    0, 1077, ...,  338,  321,   66,    9],\n",
       "        [   1,    1,    1,    1, ...,    1,    1,    1,    1],\n",
       "        [   1,    1,    1,    1, ...,    1,    1,    1,    1]]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and that's it! the data is ready for Deep Learning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
