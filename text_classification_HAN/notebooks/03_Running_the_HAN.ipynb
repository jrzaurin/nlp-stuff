{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸš¨**NOTE** ðŸš¨: to run the notebooks move them to the main dir. Simply\n",
    "\n",
    "```bash\n",
    "cp notebook_name.ipynd ../\n",
    "```\n",
    "\n",
    "So far, in notebooks 01 and 02 I have described how to prepare the data and to implement a Hierarchical Attention Network to classify amazon reviews. Here I will describe how to run the experiments. The code here is in the `main_pytorch.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from models.pytorch_models import HierAttnNet, RNNAttn\n",
    "from utils.metrics import CategoricalAccuracy\n",
    "from utils.parser import parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpus = os.cpu_count()\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\")\n",
    "train_dir = data_dir / \"train\"\n",
    "valid_dir = data_dir / \"valid\"\n",
    "test_dir = data_dir / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrain, fvalid, ftest = \"han_train.npz\", \"han_valid.npz\", \"han_test.npz\"\n",
    "tokf = \"HANPreprocessor.p\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other notebooks, and to keep the data size tractable, I will just just a sample of 1000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_mtx = np.load(train_dir / ftrain)\n",
    "np.random.seed(1)\n",
    "idx = np.random.choice(train_mtx[\"X_train\"].shape[0], 1000)\n",
    "train_set = TensorDataset(\n",
    "    torch.from_numpy(train_mtx[\"X_train\"][idx]),\n",
    "    torch.from_numpy(train_mtx[\"y_train\"][idx]).long(),\n",
    ")\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, num_workers=n_cpus)\n",
    "\n",
    "valid_mtx = np.load(valid_dir / fvalid)\n",
    "np.random.seed(2)\n",
    "idx = np.random.choice(valid_mtx[\"X_valid\"].shape[0], 1000)\n",
    "eval_set = TensorDataset(\n",
    "    torch.from_numpy(valid_mtx[\"X_valid\"][idx]),\n",
    "    torch.from_numpy(valid_mtx[\"y_valid\"][idx]).long(),\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    dataset=eval_set, batch_size=batch_size, num_workers=n_cpus, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[   1,    1,    1,  ...,   70,   88,    9],\n",
       "          [   1,    1,    1,  ...,   35, 3007,  841],\n",
       "          [   1,    1,    1,  ...,  222,   70,    9],\n",
       "          ...,\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
       " \n",
       "         [[   1,    1,    1,  ...,  109,   15,    9],\n",
       "          [   1,    1,    1,  ...,   22, 1108,    9],\n",
       "          [   1,    1,    1,  ...,  448,  144, 1587],\n",
       "          ...,\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
       " \n",
       "         [[   1,    1,    1,  ...,   10, 6706,    9],\n",
       "          [   1,    1,    1,  ...,    5,   97,    9],\n",
       "          [   1,    1,    1,  ...,   66,  186,    9],\n",
       "          ...,\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[   1,    1,    1,  ...,    6,   44,    9],\n",
       "          [   6,   59,    6,  ...,    6,  191,    9],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          ...,\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
       " \n",
       "         [[   1,    1,    1,  ...,   72,  208,    9],\n",
       "          [   1,    1,    1,  ...,   70,   28,    9],\n",
       "          [   1,    1,    1,  ...,   30,   30,   30],\n",
       "          ...,\n",
       "          [ 931, 3086, 1693,  ...,  138,  147,    9],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1]],\n",
       " \n",
       "         [[   1,    1,    1,  ...,   27,  114,    9],\n",
       "          [   1,    1,    1,  ..., 7355,   38,    9],\n",
       "          [   1,    1,    1,  ...,   19,   10,  431],\n",
       "          ...,\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1],\n",
       "          [   1,    1,    1,  ...,    1,    1,    1]]], dtype=torch.int32),\n",
       " tensor([3, 3, 3, 3, 3, 3, 0, 2, 3, 2, 3, 3, 1, 2, 3, 2, 3, 3, 1, 3, 2, 3, 0, 3,\n",
       "         3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 2, 2, 3, 3, 1, 3, 2, 3, 3, 2, 0, 3, 0,\n",
       "         3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I move on I want to add a comment the data preprocessing. One can see that the amount of padding is very significant. The default lenght that I used for both the amount of sentences in a review and the amount of tokens in a sentence is the 0.8 quantile. This, of course, implies that there is going to be a lot of padding. A priori, this does not represent a problem. In general, the network should learn that this token is irrelevant and furthermore, when using Pytorch we can pass a `padding_idx` param. This way, we pad the output with the embedding vector (normally zeros) whenever it encounters the index. Having said this, let's move on.\n",
    "\n",
    "### HAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = pickle.load(open(data_dir / tokf, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.preprocessors.HANPreprocessor at 0x13856d208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HierAttnNet(\n",
    "    vocab_size=len(tok.vocab.stoi),\n",
    "    maxlen_sent=tok.maxlen_sent,\n",
    "    maxlen_doc=tok.maxlen_doc,\n",
    "    word_hidden_dim=32,\n",
    "    sent_hidden_dim=32,\n",
    "    padding_idx=1,\n",
    "    embed_dim=50,\n",
    "    weight_drop=0.,\n",
    "    embed_drop=0.,\n",
    "    locked_drop=0.,\n",
    "    last_drop=0.,\n",
    "    embedding_matrix=None,\n",
    "    num_class=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierAttnNet(\n",
       "  (wordattnnet): WordAttnNet(\n",
       "    (lockdrop): LockedDropout()\n",
       "    (word_embed): Embedding(23611, 50, padding_idx=1)\n",
       "    (rnn): GRU(50, 32, batch_first=True, bidirectional=True)\n",
       "    (word_attn): AttentionWithContext(\n",
       "      (attn): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (contx): Linear(in_features=64, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (sentattnnet): SentAttnNet(\n",
       "    (rnn): GRU(64, 32, batch_first=True, bidirectional=True)\n",
       "    (sent_attn): AttentionWithContext(\n",
       "      (attn): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (contx): Linear(in_features=64, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (ld): Dropout(p=0.0, inplace=False)\n",
       "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I move forward let me comment on the dropout-related parameters and the implementation that one can find in the `models` module. When I started running experiments I noticed that the model overfitted quite early. In fact, in some cases, the best validation loss was attained in the very first epoch (while training loss and metrics kept improving). \n",
    "\n",
    "When overfitting occurs, one has a few options to avoid it, such as:\n",
    "\n",
    "1. Reduce model complexity\n",
    "2. Early Stop \n",
    "3. Data Augmentation \n",
    "4. Regularization (Dropout, Lable Smoothing, ...)\n",
    "\n",
    "The first one is explored throughout the different experiments I run (see notebook 04 for more details) and the second one is always used via the `early_stopping` function. Here I have ignored the 3rd one and perhaps I should use it. The reasons are two. In the first place it normally leads to good improvements and secondly, I already have the code. In the dir [amazon_reviews_classification_with_EDA](https://github.com/jrzaurin/nlp-stuff/tree/master/amazon_reviews_classification_with_EDA) I explore the use of Easy Data Augmentation ([Jason Wei and Kai Zou, 2019](https://arxiv.org/pdf/1901.11196.pdf)) to do the same as I am doing here (predict scores) but using tf-idf and topic modeling. There I describe why EDA is not particularly well suited for text processing approaches that do not consider the text as a sequence (e.g. tf-idf). Therefore, when I think about it, I am using EDA there that is not expected to lead to much improvement when I should be using it here. In any case, the code is there, so sooner or later I will bring it here and run the experiment. \n",
    "\n",
    "On the other hand, I have indeed explore regularization in the form of Dropout, lots of Dropout (if you want to have a look to Lable Smoothing for pytorch see [here](https://github.com/eladhoffer/utils.pytorch/blob/master/cross_entropy.py)). With that in mind I decided to use the Dropout implementations in the fantastic [work](https://arxiv.org/pdf/1708.02182.pdf) of Stephen Merity, Nitish Shirish Keskar and Richard Socher: Regularizing and Optimizing LSTM Language Models. There, among many other things, they discussed 3 forms of dropout: Embedding Dropout, Weight Dropout and Locked Dropout. \n",
    "\n",
    "In fact, within the `models` module there are 3 submodules named: `embed_regularize.py`, `locked_dropout.py` and `weight_dropout.py`. The code in there is **taken directly** from the original implementation at the [Salesforce repo](https://github.com/salesforce/awd-lstm-lm). The adaptations are minimal, simply adjusting the code to newer versions of `Pytorch` and a few minor style-related changes. Other than that, is a \"**copy-paste**\" of the code in their repo, so all credit to the 3 authors of the paper and the code. `last_drop` is simply dropout before the last fully connected layer.\n",
    "\n",
    "Let me comment a bit on what these dropout mechanisms do (**NOTE**: do not use the code below as it is. Use the code in the modules mentioned before).\n",
    "\n",
    "**Embedding Dropout** \n",
    "\n",
    "This is discussed in Section 4.3 in their paper and a a simplified version is shown in the following lines of code:\n",
    "\n",
    "```python\n",
    "mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(\n",
    "    1 - dropout\n",
    ").expand_as(embed.weight) / (1 - dropout)\n",
    "\n",
    "masked_embed_weight = mask * embed.weight\n",
    "```\n",
    "\n",
    "This creates a 0/1 mask along the 0-dim (i.e. words) in the `embed` Tensor and then expands that mask along the 1-dim (i.e. embed dimension). In other words, we drop words in the vocabulary with probability `dropout`. \n",
    "\n",
    "**Weight Dropout**\n",
    "\n",
    "This is discussed in Section 2 in their paper and their original implementation is [here](https://github.com/salesforce/awd-lstm-lm/blob/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5/weight_drop.py). In their own words: *\"We propose the use of DropConnect ([Wan et al., 2013](http://yann.lecun.com/exdb/publis/pdf/wan-icml-13.pdf)) on the recurrent hidden to hidden weight matrices which does not require any modifications to an RNNâ€™s formulation.\"*\n",
    "\n",
    "Again, a simplify version in code is (again, **do not** use this code as it is): \n",
    "\n",
    "```python\n",
    "class WeightDrop(nn.Module):\n",
    "    def __init__(self, module, weights, dropout=0, verbose=True):\n",
    "        super(WeightDrop, self).__init__()\n",
    "        self.module = module\n",
    "        self.weights = weights\n",
    "        self.dropout = dropout        \n",
    "        self.verbose = verbose\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self):\n",
    "        for name_w in self.weights:\n",
    "            if self.verbose:\n",
    "                print(\"Applying weight drop of {} to {}\".format(self.dropout, name_w))\n",
    "            w = getattr(self.module, name_w)\n",
    "            del self.module._parameters[name_w]\n",
    "            self.module.register_parameter(name_w + \"_raw\", nn.Parameter(w.data))\n",
    "\n",
    "    def _setweights(self):\n",
    "        for name_w in self.weights:\n",
    "            raw_w = getattr(self.module, name_w + \"_raw\")\n",
    "            w = nn.Parameter(\n",
    "                torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n",
    "            )\n",
    "            setattr(self.module, name_w, w)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module.forward(*args)\n",
    "```\n",
    "\n",
    "Let's see what this does. `WeightDrop` will first copy and register the hidden-to-hidden weights (or in general terms the weights in the `List` weights) with a suffix `_raw`.  Then, we apply dropout and assign the weights again to the `module`. Their original implementation includes a so-called `variational` version also explained in the paper (please, read the paper). \n",
    "\n",
    "This implementation has a couple of drawbacks that I will discuss in the next notebooks. There are implementations of these dropout mechanisms other than the original one discussed here (but inspired by that one, of course). For example, the `text` API at the `fastai` library has a very neat [implementation](https://github.com/fastai/fastai/blob/master/fastai/text/models/awd_lstm.py#L75). Another nice [implemenation](https://github.com/dmlc/gluon-nlp/blob/8869e795b683ff52073b556cd24e1d06cf9952ac/src/gluonnlp/model/utils.py#L34) is found at the `Mxnet`'s `gluonnlp` library, which I have also used here **$-$ although only the `Pytorch` implementation is discussed here, I have also implemented the full HAN model using `Mxnet` $-$**. \n",
    "\n",
    "And finally\n",
    "\n",
    "**Locked Dropout**\n",
    "\n",
    "Simply:\n",
    "\n",
    "```python\n",
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, dropout=0.5):\n",
    "        if not self.training or not dropout:\n",
    "            return x\n",
    "        mask = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout) / (1 - dropout)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x\n",
    "```\n",
    "\n",
    "After having explained the previous two mechanisms, this does not require much explanation. Quickly, this generates a mask long the 1st-dim of the 3-dim input Tensor and expands that mask along the 0-dim. For example, when applied to a Tensor like `(batch_size, seq_length, embed_dim)`, it will create a mask of dim `(1, seq_length, embed_dim)` and apply it to the whole batch.\n",
    "\n",
    "Once we have the model, we need the remaining Pytorch components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "# This class is at the utils module\n",
    "metric = CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the standard train and validation steps, along with an early stop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, train_loader, epoch, metric):\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    train_steps = len(train_loader)\n",
    "    running_loss = 0\n",
    "    with trange(train_steps) as t:\n",
    "        for batch_idx, (data, target) in zip(t, train_loader):\n",
    "            t.set_description(\"epoch %i\" % (epoch + 1))\n",
    "\n",
    "            X = data.cuda() if use_cuda else data\n",
    "            y = target.cuda() if use_cuda else target\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            acc = metric(F.softmax(y_pred, dim=1), y)\n",
    "\n",
    "            t.set_postfix(acc=acc, loss=avg_loss)\n",
    "\n",
    "\n",
    "def eval_step(model, eval_loader, metric, is_test=False):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    eval_steps = len(eval_loader)\n",
    "    running_loss = 0\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        with trange(eval_steps) as t:\n",
    "            for batch_idx, (data, target) in zip(t, eval_loader):\n",
    "                if is_test:\n",
    "                    t.set_description(\"test\")\n",
    "                else:\n",
    "                    t.set_description(\"valid\")\n",
    "\n",
    "                X = data.cuda() if use_cuda else data\n",
    "                y = target.cuda() if use_cuda else target\n",
    "\n",
    "                y_pred = model(X)\n",
    "                loss = F.cross_entropy(y_pred, y)\n",
    "                running_loss += loss.item()\n",
    "                avg_loss = running_loss / (batch_idx + 1)\n",
    "                acc = metric(F.softmax(y_pred, dim=1), y)\n",
    "                if is_test:\n",
    "                    preds.append(y_pred)\n",
    "                t.set_postfix(acc=acc, loss=avg_loss)\n",
    "\n",
    "    return avg_loss, preds\n",
    "\n",
    "\n",
    "def early_stopping(curr_value, best_value, stop_step, patience):\n",
    "    if curr_value <= best_value:\n",
    "        stop_step, best_value = 0, curr_value\n",
    "    else:\n",
    "        stop_step += 1\n",
    "    if stop_step >= patience:\n",
    "        print(\"Early stopping triggered. log:{}\".format(best_value))\n",
    "        stop = True\n",
    "    else:\n",
    "        stop = False\n",
    "    return best_value, stop_step, stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that we are good to go. Note that one could just define the train/eval functions so that they run all the epochs. Normally I prefer to code the steps and run them in a loop. A matter of taste, also depends on the code structure. In this particular case, I will leave it as it is. \n",
    "\n",
    "To run the model simply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.73it/s, acc=0.542, loss=1.25]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 18.30it/s, acc=0.577, loss=1.15]\n",
      "epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.76it/s, acc=0.584, loss=1.12]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 18.35it/s, acc=0.577, loss=1.12]\n",
      "epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.73it/s, acc=0.584, loss=1.09]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 18.47it/s, acc=0.577, loss=1.11]\n",
      "epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.77it/s, acc=0.584, loss=1.08]\n",
      "valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 18.60it/s, acc=0.577, loss=1.09]\n"
     ]
    }
   ],
   "source": [
    "metric = CategoricalAccuracy()\n",
    "n_epochs = 4\n",
    "eval_every = 1\n",
    "patience = 1\n",
    "stop_step = 0\n",
    "best_loss = 1e6\n",
    "for epoch in range(n_epochs):\n",
    "    train_step(model, optimizer, train_loader, epoch, metric)\n",
    "    if epoch % eval_every == (eval_every - 1):\n",
    "        val_loss, _ = eval_step(model, eval_loader, metric)\n",
    "        best_loss, stop_step, stop = early_stopping(\n",
    "            val_loss, best_loss, stop_step, patience\n",
    "        )\n",
    "    if stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. If you have a look to the `main_pytorch.py` script you will see that, of course, there are a number of additional adds on to the training/validation/test process, but the main bits and pieces have been discussed here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
