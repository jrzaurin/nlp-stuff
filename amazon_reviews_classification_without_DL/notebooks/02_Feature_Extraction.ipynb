{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the features that will be fed to LightGBM for the amazon review classification problem. Before I start, just mentioning that prior to the code in this notebook I have run the `train_test_split.py`. As a result, there are 3 sub-directories within the `data` directory that contain the train, validation and test splits.\n",
    "\n",
    "The algorithms I am going to use here to \"extract\" the features are: \n",
    "\n",
    "- Our good friend tf-idf\n",
    "- The well known LDA topic modeling algorithm\n",
    "- And the promising EnsTop package, an ensemble based approach to topic modelling using pLSA\n",
    "\n",
    "Let me write a bit more about the later technique. The `Enstop` package was created by [Leland McInnes](https://github.com/lmcinnes) and [Gökçen Eraslan](https://github.com/gokceneraslan). Leland McInnes is also the creator of [UMAP](https://github.com/lmcinnes/umap) (Uniform Manifold Approximation and Projection). When I first saw his youtube [presentation](https://www.youtube.com/watch?v=nq6iPZVUxZU&t=8s) on UMAP, it made me want to go back to uni and do a PhD or MPhil in topology. Seriously, check it out. \n",
    "\n",
    "Coming back to `Enstop`, the package is, as explained by the authors, an ensemble based approach to topic modelling using pLSA. Leaving aside the use of `numba` for high perfomance, the package runs multiple topic models using pLSA, and then clusters them using HDBSCAN to determine a set of stable topics. If you have a look to the source code of the package, you will see that the way they compute the stable topics can be described as follows:\n",
    "\n",
    "- Compute an ensemble of topics using their pLSA implementation of NMF. For example, if you run 16 experiments (their default), this step will result in a `(n_runs * n_topics, n_words)` array\n",
    "- Once we have that large array, they select a small list of stable topics using 3 optional methods:\n",
    "    - HDBSCAN and the KL-divergence as a distance\n",
    "    - HDBSCAN and Hellinger as a distance\n",
    "    - First projecting the topics onto a low dimensional space using UMAP with the Hellinger distance. The cluster the topics using HDBSCAN and the euclidean distance in the lower-dim space. This is their default method. \n",
    "    \n",
    "Of course, there is more to it, since the package comes with its own topic coherence meassure and a few more rings and bells. However, there is a notable caveat/drawback. `UMAP` does not work with sparse matrices through the entire fit and trasnform pipeline (e.g. see [here](https://github.com/lmcinnes/umap/issues/81)). These will be move to dense during the process. \n",
    "\n",
    "Therefore, when using `Enstop` with the default options, the number of topics must be small, if not the memory explodes. For example, using an AWS c5.4xlarge EC2 instance (16 cores, 30.4 GB) with ouramazon reviews dataset, I can only use 20 topics, and that already uses 30GB of RAM. \n",
    "\n",
    "I have not tried to use the other two options described before (KL-divergence and Hellinger), but I do hope UMAP supports sparse matrices in the near future because its potential is significant. \n",
    "\n",
    "Anyway, without further ado, let's move to the feature extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from enstop import EnsembleTopics\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cores = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction(object):\n",
    "    def __init__(self, algo, n_topics=None, max_vocab_size=50000):\n",
    "        super(FeatureExtraction, self).__init__()\n",
    "\n",
    "        if algo is 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(max_features=max_vocab_size, preprocessor = lambda x: x,\n",
    "                tokenizer = lambda x: x)\n",
    "            self.fe = Pipeline([('vectorizer', vectorizer)])\n",
    "        else:\n",
    "            assert n_topics is not None\n",
    "            vectorizer = CountVectorizer(max_features=max_vocab_size, preprocessor = lambda x: x,\n",
    "                tokenizer = lambda x: x)\n",
    "            if algo is 'lda':\n",
    "                model = LDA(n_components=n_topics, n_jobs=-1, random_state=0)\n",
    "            elif algo is 'ensemb':\n",
    "                model = EnsembleTopics(n_components=n_topics, n_jobs=cores, random_state=0)\n",
    "            self.fe = Pipeline([('vectorizer', vectorizer), ('model', model)])\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.fe.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        out = self.fe.transform(X)\n",
    "        return out\n",
    "\n",
    "    def fit_transform(self,X):\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already splitted the dataset in training, validation and testing, check the `train_test_split.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'nltk_tok_reviews'\n",
    "algo='tfidf'\n",
    "n_topics=None\n",
    "max_vocab_size=20000\n",
    "root='../data'\n",
    "train_dir='train' \n",
    "valid_dir='valid'\n",
    "test_dir='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = Path('/'.join([root, train_dir]))\n",
    "VALID_PATH = Path('/'.join([root, valid_dir]))\n",
    "TEST_PATH  = Path('/'.join([root, test_dir]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pickle.load(open(TRAIN_PATH/(dataset+'_tr.p'), 'rb'))\n",
    "dvalid = pickle.load(open(VALID_PATH/(dataset+'_val.p'), 'rb'))\n",
    "dtest  = pickle.load(open(TEST_PATH/(dataset+'_te.p'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are sklearn's data bunches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'y'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buy', 'husband', 'have', 'leg', 'pain', 'vericose', 'vein', 'purchase', 'heavier', 'expensive', 'black', 'sock', 'seeif', 'help', 'ease', 'discomfort', 'work', 'stand', 'day', 'help', 'heavy', 'warm', 'summer', 'end', 'buy', 'twopairs', 'try', 'help', 'great', 'deal', 'eventually', 'purchase', 'pair', 'win', 'wear', 'dayand', 'machine', 'wash', 'air', 'dry', 'go', 'dryer', 'time', 'count', 'affect', 'fit', 'don', 'roll', 'like', 'match', 'sock', 'instead', 'fold', 'drawer', 'stretch', 'pretty', 'frugaland', 'search', 'better', 'price', 'quality', 'couldn', 'find', 'better', 'think', 'expensive', 'worth', 'themoney']\n"
     ]
    }
   ],
   "source": [
    "print(dtrain.X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And simply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtraction(algo, n_topics, max_vocab_size)\n",
    "X_tr  = feature_extractor.fit_transform(dtrain.X)\n",
    "X_val = feature_extractor.transform(dvalid.X)\n",
    "X_te  = feature_extractor.transform(dtest.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<222906x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4724123 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And change the settings accordingly to use `LDA` or `EnsembleTopics`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
